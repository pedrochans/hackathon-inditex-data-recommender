{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook para responder las preguntas de la Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de los DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path    \n",
    "import pandas as pd\n",
    "\n",
    "RAW_DATA_PATH = Path('../../data/raw')\n",
    "\n",
    "train_df = pd.read_csv(         RAW_DATA_PATH / 'train.csv')\n",
    "test_df = pd.read_csv(          RAW_DATA_PATH / 'test.csv')\n",
    "products_df = pd.read_pickle(   RAW_DATA_PATH / 'products.pkl')\n",
    "users_df = pd.read_csv(         RAW_DATA_PATH / 'users_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Extraer datos del string (JSON) de la columna 'values'\n",
    "def extract_data_from_string(df, column_name):\n",
    "    \n",
    "    df[column_name] = df[column_name].apply(ast.literal_eval)\n",
    "    \n",
    "    df['country'] = df[column_name].apply(lambda x: x['country'])\n",
    "    df['R'] = df[column_name].apply(lambda x: x['R'])\n",
    "    df['F'] = df[column_name].apply(lambda x: x['F'])\n",
    "    df['M'] = df[column_name].apply(lambda x: x['M'])\n",
    "    \n",
    "    df = df.drop(columns=[column_name])\n",
    "    \n",
    "    df = df.explode(['country', 'R', 'F', 'M'])\n",
    "    \n",
    "    df['country'] = df['country'].astype(int)\n",
    "    df['R'] = df['R'].astype(int)\n",
    "    df['F'] = df['F'].astype(int)\n",
    "    df['M'] = df['M'].astype(float)\n",
    "    \n",
    "    return df\n",
    "\n",
    "users_df = extract_data_from_string(users_df, 'values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 1:** Answer the following questions and develop two functions about the train, clients and products datasets:\n",
    "  - **Q1:** Which product (`partnumber`) with `color_id` equal to 3   belongs to the lowest `family` code with a `discount`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "      discount                                          embedding  partnumber  \\\n",
      "8557         0  [-0.13718931, 0.29018262, -0.11451288, -0.1793...       39798   \n",
      "8558         0  [-0.09487282, -0.14113918, -0.14696811, -0.191...       38018   \n",
      "8559         0  [0.19879724, -0.21834055, -0.030612089, -0.054...       36259   \n",
      "8561         0  [-0.07990676, -0.005529883, -0.13825129, -0.09...       38933   \n",
      "19413        1  [-0.16633523, -0.13765946, 0.31306392, -0.1556...       17265   \n",
      "\n",
      "       color_id  cod_section  family  \n",
      "8557          3          4.0       1  \n",
      "8558          3          4.0       1  \n",
      "8559          3          4.0       1  \n",
      "8561          3          4.0       1  \n",
      "19413         3          4.0       1  \n",
      "1\n",
      "      discount                                          embedding  partnumber  \\\n",
      "19413        1  [-0.16633523, -0.13765946, 0.31306392, -0.1556...       17265   \n",
      "\n",
      "       color_id  cod_section  family  \n",
      "19413         3          4.0       1  \n",
      "17265\n"
     ]
    }
   ],
   "source": [
    "# Productos con descuento\n",
    "discounted_products = products_df[(products_df['discount']==1)]\n",
    "\n",
    "# Menor codigo de familia con descuento\n",
    "min_fam_desc = discounted_products['family'].min()\n",
    "\n",
    "# Productos cuyo codigo de familia es el menor con descuento\n",
    "min_fam_desc_products = products_df[products_df['family']==min_fam_desc]\n",
    "\n",
    "# Del subconjunto anterior, aquel cuyo color_id = 3\n",
    "target = min_fam_desc_products[min_fam_desc_products['color_id']==3]\n",
    "\n",
    "print(len(target))\n",
    "print(target.head())\n",
    "# Siendo estrictos con el enunciado, estos son los 34 productos que verifican tener color_id = 3 y pertenecer al menor código de familia con descuento (family = 1)\n",
    "\n",
    "target_2 = products_df[(products_df['discount']==1) & (products_df['family']==min_fam_desc) & (products_df['color_id']==3)]\n",
    "print(len(target_2))\n",
    "print(target_2)\n",
    "# Siendo menos estrictos, y entendiendo que la condición de tener descuento hay que aplicarla también sobre el resultado final, (discount=1), entonces tendríamos 2 productos:\n",
    "\n",
    "# No existe un resultado único para la pregunta descrita, por lo que escogemos como resultado el primer partnumber 18091 , que verifica ambas interpretaciones de la pregunta.\n",
    "Q1 = target_2.iloc[0]['partnumber']\n",
    "print(Q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Q2:** In the country where most users have made purchases totaling less than 500 (`M`) , which is the user who has the lowest purchase frequency (`F`), the most recent purchase (highest `R`) and the lowest `user_id`? Follow the given order of variables as the sorting priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country with most users having purchases <500: 25\n",
      "187374\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el total por usuaro de M, F y R,\n",
    "unique_users_df = users_df.groupby(['user_id', 'country']).agg({\n",
    "    'M': 'sum',         # Sum purchases per country\n",
    "    'F': 'sum',         # Sum frequency per country\n",
    "    'R': 'max'          # Most recent per country\n",
    "}).reset_index()\n",
    "\n",
    "# Obtenemos el país con mayor número de usuarios que tienen < 500 en la columna M (compras)\n",
    "unique_users_df['less_500_purchases'] = unique_users_df['M'] < 500\n",
    "country_agg = unique_users_df.groupby('country', as_index=False)['less_500_purchases'].sum()\n",
    "target_country = country_agg.loc[country_agg['less_500_purchases'].idxmax(), 'country']\n",
    "print(f'Country with most users having purchases <500: {target_country}')\n",
    "\n",
    "# Filtramos el país obtenido anteriormente y ordenamos los usuarios por F (menor a mayor), R (mayor a menor) y user_id (menor a mayor).\n",
    "users_df_c = unique_users_df[unique_users_df['country'] == target_country]\n",
    "sorted_user = users_df_c.sort_values(\n",
    "    by=['F', 'R', 'user_id'], \n",
    "    ascending=[True, False, True]\n",
    ").iloc[0]\n",
    "\n",
    "Q2 = sorted_user['user_id']\n",
    "print(Q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Q3:** Among the products that were added to the cart at least once, how many times is a product visited before it is added to the cart in average? Give the answer with 2 decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos los productos que han sido añadidos al carrito al menos una vez\n",
    "products_subset = train_df[train_df['add_to_cart']==True]['partnumber'].unique()\n",
    "train_df_s = train_df[train_df['partnumber'].isin(products_subset)]\n",
    "\n",
    "# Agrupamos y contamos registros (visitas o interacciones) y sumamos las veces que se ha añadido al carrito\n",
    "product_stats = train_df_s.groupby('partnumber').agg({\n",
    "    'add_to_cart': ['sum','count']\n",
    "}).reset_index()\n",
    "product_stats.columns = ['partnumber', 'cart_adds', 'total_visits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.69\n"
     ]
    }
   ],
   "source": [
    "# Cálculo\n",
    "product_stats['visits_before_cart'] = (product_stats['total_visits']) / product_stats['cart_adds']\n",
    "\n",
    "Q3 = round(product_stats['visits_before_cart'].mean(),2) \n",
    "print(Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Q4:** Which device (`device_type`) is most frequently used by users to make purchases (`add_to_cart` = 1) of discounted products (`discount` = 1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Filtramos los datasets por separado y luego resolvemos el inner join\n",
    "train_df_added_cart = train_df[train_df['add_to_cart']==True]\n",
    "discounted_products_df = products_df[products_df['discount'] == 1]\n",
    "result_df = train_df_added_cart.merge(discounted_products_df[['partnumber']], on='partnumber', how='inner')[['device_type']]\n",
    "\n",
    "# Obtenemos la moda (valor más frecuente) de la columna 'device_type' en el resultado\n",
    "Q4 = result_df['device_type'].mode()[0]\n",
    "print(Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Q5:** Among users with purchase frequency (`F`) in the top 3 within their purchase country, who has interacted with the most products (`partnumber`) in sessions conducted from a device with identifier 3 (`device_type` = 3)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chans\\AppData\\Local\\Temp\\ipykernel_2388\\878451378.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_3_per_country = users_df.groupby('country').apply(lambda x: x.nlargest(3, 'F')).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72153\n"
     ]
    }
   ],
   "source": [
    "# Obtener los 3 usuarios con mayor valor de F para cada valor de country\n",
    "top_3_per_country = users_df.groupby('country').apply(lambda x: x.nlargest(3, 'F')).reset_index(drop=True)\n",
    "\n",
    "# Filtramos los usuarios en el dataset de interacciones, y filtramos que sean sesiones en device_type = 3\n",
    "target_users = top_3_per_country['user_id'].unique()\n",
    "train_df_a = train_df[(train_df['user_id'].isin(target_users)) & (train_df['device_type'] == 3)]\n",
    "result_df = train_df_a.groupby('user_id').agg({\n",
    "    'partnumber' : 'nunique'\n",
    "}).sort_values('partnumber', ascending=False)\n",
    "\n",
    "# Obtenemos el user_id que ha interaccionado con más productos\n",
    "Q5 = int(result_df.index[0])  \n",
    "print(Q5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Q6:** For interactions that occurred outside the user's country of residence, how many unique family identifiers are there? Take into account any registered country for each user, as there may be more than one country per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6856730\n",
      "14257\n"
     ]
    }
   ],
   "source": [
    "# Hacemos Join con los usuarios y sus países de residencia y nos quedamos con las filas que no cruzan\n",
    "train_df_b = train_df[train_df['user_id'].notna()].merge(users_df[['user_id', 'country']], \n",
    "                                on=['user_id', 'country'], \n",
    "                                how='left',\n",
    "                                indicator=True)\n",
    "\n",
    "print(len(train_df_b))\n",
    "\n",
    "train_df_b = train_df_b[train_df_b['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "\n",
    "print(len(train_df_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_c = train_df_b.merge(products_df[['partnumber', 'family']], \n",
    "                                on=['partnumber'], \n",
    "                                how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "Q6 = train_df_c['family'].nunique() \n",
    "print(Q6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique families in cross-border interactions: 118\n"
     ]
    }
   ],
   "source": [
    "# 1. Join train_df with users_df to get user countries\n",
    "cross_border_df = train_df[train_df['user_id'].notna()].merge(\n",
    "    users_df[['user_id', 'country']], \n",
    "    on='user_id',\n",
    "    how='left',\n",
    "    suffixes=('_interaction', '_user')\n",
    ")\n",
    "\n",
    "# 2. Filter for cross-border interactions\n",
    "cross_border_df = cross_border_df[\n",
    "    cross_border_df['country_interaction'] != cross_border_df['country_user']\n",
    "]\n",
    "\n",
    "# 3. Join with products_df to get family info\n",
    "cross_border_df = cross_border_df.merge(\n",
    "    products_df[['partnumber', 'family']], \n",
    "    on='partnumber',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 4. Count unique families\n",
    "Q6 = cross_border_df['family'].nunique()\n",
    "print(f\"Number of unique families in cross-border interactions: {Q6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Q7:** Among interactions from the first 7 days of June, which is the most frequent page type where each family is added to the cart? Return it in the following format: `{'('family'): int('most_frequent_pagetype')}` . In case of a tie, return the smallest pagetype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_d = train_df[(train_df['date'] >= '2024-06-01') & (train_df['date'] <= '2024-06-07') & (train_df['add_to_cart'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 24, '2': 24, '3': 24, '4': 24, '5': 24, '6': 24, '7': 24, '8': 24, '9': 24, '10': 24, '11': 24, '12': 24, '13': 24, '14': 24, '15': 24, '16': 24, '17': 24, '18': 24, '19': 24, '21': 24, '22': 24, '23': 24, '24': 24, '26': 24, '27': 24, '28': 24, '29': 24, '30': 24, '31': 24, '32': 24, '33': 24, '34': 24, '35': 24, '36': 24, '37': 24, '38': 24, '40': 24, '41': 24, '42': 24, '43': 24, '44': 24, '45': 24, '46': 24, '47': 24, '48': 24, '49': 24, '50': 24, '51': 24, '52': 24, '53': 24, '54': 24, '56': 24, '57': 24, '58': 24, '59': 24, '60': 24, '61': 24, '62': 24, '63': 24, '64': 24, '65': 24, '66': 24, '67': 24, '68': 24, '69': 24, '70': 24, '71': 24, '72': 24, '73': 24, '74': 24, '75': 24, '76': 24, '77': 24, '78': 24, '79': 24, '81': 24, '82': 24, '83': 24, '84': 24, '85': 24, '86': 24, '87': 24, '88': 24, '89': 24, '90': 24, '91': 24, '92': 24, '93': 24, '94': 24, '95': 24, '96': 24, '97': 24, '99': 24, '100': 24, '101': 24, '102': 24, '103': 24, '104': 24, '105': 24, '106': 24, '107': 24, '108': 24, '109': 24, '110': 24, '111': 24, '112': 24, '113': 24, '114': 24, '115': 24, '116': 24, '117': 24, '118': 24, '119': 24, '120': 24, '121': 24, '122': 24, '123': 24, '124': 24, '125': 24, '126': 24, '127': 24, '128': 24, '129': 24, '130': 24, '131': 24, '132': 24, '133': 24, '134': 24, '135': 24, '136': 24, '137': 24, '138': 24, '139': 24, '140': 24, '141': 24, '142': 24, '143': 24, '144': 24, '145': 24, '146': 24, '147': 24, '148': 24, '149': 24, '150': 24, '151': 24, '152': 24, '153': 24, '154': 24, '155': 24, '156': 24, '157': 24, '158': 24, '159': 24, '160': 24, '161': 24, '162': 24, '163': 24, '164': 24, '165': 24, '166': 24, '167': 24, '168': 24, '169': 24, '170': 24, '171': 24, '172': 24, '173': 24, '174': 24, '175': 24, '177': 24, '179': 24, '180': 24, '181': 24, '182': 24, '183': 24, '184': 24, '185': 24, '186': 24, '187': 24, '188': 24, '189': 24, '190': 24, '191': 24, '192': 24, '193': 24, '194': 24, '195': 24, '196': 24, '197': 24, '199': 24, '200': 24, '201': 24, '202': 24, '203': 24, '205': 24, '206': 24, '207': 24, '208': 19, '209': 24, '210': 24, '211': 24, '214': 24}\n"
     ]
    }
   ],
   "source": [
    "train_df_e = train_df_d.merge(products_df[['partnumber', 'family']], \n",
    "    on=['partnumber'], \n",
    "    how='inner')\n",
    "\n",
    "# Agrupar por familia y tipo de pagina y contar las apariciones, ordenar descendente\n",
    "train_df_f = train_df_e.groupby(['family','pagetype']).size().reset_index(name='count')\n",
    "train_df_f = train_df_f.sort_values('count', ascending=False)\n",
    "\n",
    "# Crear el diccionaro objetivo\n",
    "Q7 = {}\n",
    "for family in sorted(train_df_f['family'].unique()):\n",
    "    family_data = train_df_f[train_df_f['family'] == family]\n",
    "    # Ordenar por apariciones descendente, y por pagetype ascendente (caso de empate)\n",
    "    most_common = family_data.sort_values(['count', 'pagetype'], ascending=[False, True]).iloc[0]\n",
    "    Q7[str(family)] = int(most_common['pagetype'])\n",
    "\n",
    "print(Q7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entregamos en el formato deseado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert numpy types to native Python types\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    return obj\n",
    "\n",
    "# Create predictions dictionary\n",
    "predictions = {\n",
    "    \"target\": {\n",
    "        \"query_1\": {\n",
    "            \"partnumber\": int(Q1)\n",
    "        },\n",
    "        \"query_2\": {\n",
    "            \"user_id\": int(Q2)\n",
    "        },\n",
    "        \"query_3\": {\n",
    "            \"average_previous_visits\": float(Q3)\n",
    "        },\n",
    "        \"query_4\": {\n",
    "            \"device_type\": int(Q4)\n",
    "        },\n",
    "        \"query_5\": {\n",
    "            \"user_id\": int(Q5)\n",
    "        },\n",
    "        \"query_6\": {\n",
    "            \"unique_families\": int(Q6)\n",
    "        },\n",
    "        \"query_7\": convert_to_serializable(Q7) \n",
    "    }\n",
    "}\n",
    "\n",
    "# Create predictions directory if it doesn't exist\n",
    "os.makedirs('predictions', exist_ok=True)\n",
    "\n",
    "# Save to JSON file with proper formatting\n",
    "with open('../../predictions/predictions_1.json', 'w') as f:\n",
    "    json.dump(predictions, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_query_answers(train_df, products_df, users_df):\n",
    "    # Query 1\n",
    "    q1_result = products_df[\n",
    "        (products_df['color_id'] == 3) & \n",
    "        (products_df['discount'] == True)\n",
    "    ].nsmallest(1, 'family')['partnumber'].iloc[0]\n",
    "\n",
    "    # Query 2\n",
    "    country_counts = users_df[users_df['M'] < 500].groupby('country').size()\n",
    "    target_country = country_counts.idxmax()\n",
    "    \n",
    "    q2_result = users_df[users_df['country'] == target_country].sort_values(\n",
    "        by=['F', 'R', 'user_id'], \n",
    "        ascending=[True, False, True]\n",
    "    )['user_id'].iloc[0]\n",
    "\n",
    "    # Query 3\n",
    "    cart_added = train_df[train_df['add_to_cart'] == 1]['partnumber'].unique()\n",
    "    visits_before_cart = []\n",
    "    \n",
    "    for prod in cart_added:\n",
    "        prod_interactions = train_df[train_df['partnumber'] == prod].sort_values('timestamp_local')\n",
    "        first_cart = prod_interactions[prod_interactions['add_to_cart'] == 1].index[0]\n",
    "        visits = len(prod_interactions.loc[:first_cart]) - 1\n",
    "        if visits > 0:\n",
    "            visits_before_cart.append(visits)\n",
    "    \n",
    "    q3_result = round(sum(visits_before_cart) / len(visits_before_cart), 2)\n",
    "\n",
    "    # Query 4\n",
    "    cart_purchases = train_df[\n",
    "        (train_df['add_to_cart'] == 1) & \n",
    "        (train_df['partnumber'].isin(products_df[products_df['discount'] == 1]['partnumber']))\n",
    "    ]\n",
    "    q4_result = cart_purchases['device_type'].mode().iloc[0]\n",
    "\n",
    "    # Query 5\n",
    "    top3_freq = users_df.groupby('country')['F'].nlargest(3).reset_index()\n",
    "    top_users = users_df[users_df['user_id'].isin(top3_freq['user_id'])]\n",
    "    \n",
    "    device3_interactions = train_df[\n",
    "        (train_df['device_type'] == 3) & \n",
    "        (train_df['user_id'].isin(top_users['user_id']))\n",
    "    ]\n",
    "    \n",
    "    q5_result = device3_interactions.groupby('user_id')['partnumber'].nunique().idxmax()\n",
    "\n",
    "    # Query 6\n",
    "    user_countries = users_df.groupby('user_id')['country'].apply(list)\n",
    "    outside_interactions = train_df[~train_df.apply(\n",
    "        lambda x: x['country'] in user_countries.get(x['user_id'], []), axis=1\n",
    "    )]\n",
    "    \n",
    "    q6_result = len(products_df[\n",
    "        products_df['partnumber'].isin(outside_interactions['partnumber'])\n",
    "    ]['family'].unique())\n",
    "\n",
    "    # Query 7\n",
    "    june_df = train_df[\n",
    "        (pd.to_datetime(train_df['date']).dt.month == 6) & \n",
    "        (pd.to_datetime(train_df['date']).dt.day <= 7) & \n",
    "        (train_df['add_to_cart'] == 1)\n",
    "    ]\n",
    "    \n",
    "    family_pages = {}\n",
    "    for family in products_df['family'].unique():\n",
    "        family_prods = products_df[products_df['family'] == family]['partnumber']\n",
    "        if len(june_df[june_df['partnumber'].isin(family_prods)]) > 0:\n",
    "            family_pages[str(family)] = int(\n",
    "                june_df[june_df['partnumber'].isin(family_prods)]['pagetype'].mode().iloc[0]\n",
    "            )\n",
    "\n",
    "    result = {\n",
    "        \"target\": {\n",
    "            \"query_1\": {\"partnumber\": int(q1_result)},\n",
    "            \"query_2\": {\"user_id\": int(q2_result)},\n",
    "            \"query_3\": {\"average_previous_visits\": float(q3_result)},\n",
    "            \"query_4\": {\"device_type\": int(q4_result)},\n",
    "            \"query_5\": {\"user_id\": int(q5_result)},\n",
    "            \"query_6\": {\"unique_families\": int(q6_result)},\n",
    "            \"query_7\": family_pages\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Save results to JSON file\n",
    "def save_results(results):\n",
    "    with open('predictions/predictions_1.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get and save results\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_query_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproducts_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musers_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m save_results(results)\n",
      "Cell \u001b[1;32mIn[37], line 27\u001b[0m, in \u001b[0;36mget_query_answers\u001b[1;34m(train_df, products_df, users_df)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prod \u001b[38;5;129;01min\u001b[39;00m cart_added:\n\u001b[0;32m     26\u001b[0m     prod_interactions \u001b[38;5;241m=\u001b[39m train_df[train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartnumber\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m prod]\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp_local\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     first_cart \u001b[38;5;241m=\u001b[39m prod_interactions[\u001b[43mprod_interactions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madd_to_cart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m]\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     28\u001b[0m     visits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(prod_interactions\u001b[38;5;241m.\u001b[39mloc[:first_cart]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m visits \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\chans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32mc:\\Users\\chans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:347\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_cmp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32mc:\\Users\\chans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    215\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(expressions\u001b[38;5;241m.\u001b[39mevaluate, op)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m    222\u001b[0m     ):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_str \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[1;32mc:\\Users\\chans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TEST_MODE:\n\u001b[0;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "\n",
    "# Get and save results\n",
    "results = get_query_answers(train_df, products_df, users_df)\n",
    "save_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
