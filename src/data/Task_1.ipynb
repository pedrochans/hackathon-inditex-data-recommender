{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook para responder las preguntas de la Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de los DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path    \n",
    "import pandas as pd\n",
    "\n",
    "RAW_DATA_PATH = Path('../../data/raw')\n",
    "\n",
    "train_df = pd.read_csv(         RAW_DATA_PATH / 'train.csv')\n",
    "test_df = pd.read_csv(          RAW_DATA_PATH / 'test.csv')\n",
    "products_df = pd.read_pickle(   RAW_DATA_PATH / 'products.pkl')\n",
    "users_df = pd.read_csv(         RAW_DATA_PATH / 'users_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Extraer datos del string (JSON) de la columna 'values'\n",
    "def extract_data_from_string(df, column_name):\n",
    "    \n",
    "    df[column_name] = df[column_name].apply(ast.literal_eval)\n",
    "    \n",
    "    df['country'] = df[column_name].apply(lambda x: x['country'])\n",
    "    df['R'] = df[column_name].apply(lambda x: x['R'])\n",
    "    df['F'] = df[column_name].apply(lambda x: x['F'])\n",
    "    df['M'] = df[column_name].apply(lambda x: x['M'])\n",
    "    \n",
    "    df = df.drop(columns=[column_name])\n",
    "    \n",
    "    df = df.explode(['country', 'R', 'F', 'M'])\n",
    "    \n",
    "    df['country'] = df['country'].astype(int)\n",
    "    df['R'] = df['R'].astype(int)\n",
    "    df['F'] = df['F'].astype(int)\n",
    "    df['M'] = df['M'].astype(float)\n",
    "    \n",
    "    return df\n",
    "\n",
    "users_df = extract_data_from_string(users_df, 'values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 1:** Answer the following questions and develop two functions about the train, clients and products datasets:\n",
    "  - **Q1:** Which product (`partnumber`) with `color_id` equal to 3   belongs to the lowest `family` code with a `discount`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "      discount                                          embedding  partnumber  \\\n",
      "8557         0  [-0.13718931, 0.29018262, -0.11451288, -0.1793...       39798   \n",
      "8558         0  [-0.09487282, -0.14113918, -0.14696811, -0.191...       38018   \n",
      "8559         0  [0.19879724, -0.21834055, -0.030612089, -0.054...       36259   \n",
      "8561         0  [-0.07990676, -0.005529883, -0.13825129, -0.09...       38933   \n",
      "19413        1  [-0.16633523, -0.13765946, 0.31306392, -0.1556...       17265   \n",
      "\n",
      "       color_id  cod_section  family  \n",
      "8557          3          4.0       1  \n",
      "8558          3          4.0       1  \n",
      "8559          3          4.0       1  \n",
      "8561          3          4.0       1  \n",
      "19413         3          4.0       1  \n",
      "1\n",
      "      discount                                          embedding  partnumber  \\\n",
      "19413        1  [-0.16633523, -0.13765946, 0.31306392, -0.1556...       17265   \n",
      "\n",
      "       color_id  cod_section  family  \n",
      "19413         3          4.0       1  \n",
      "17265\n"
     ]
    }
   ],
   "source": [
    "# Productos con descuento\n",
    "discounted_products = products_df[(products_df['discount']==1)]\n",
    "\n",
    "# Menor codigo de familia con descuento\n",
    "min_fam_desc = discounted_products['family'].min()\n",
    "\n",
    "# Productos cuyo codigo de familia es el menor con descuento\n",
    "min_fam_desc_products = products_df[products_df['family']==min_fam_desc]\n",
    "\n",
    "# Del subconjunto anterior, aquel cuyo color_id = 3\n",
    "target = min_fam_desc_products[min_fam_desc_products['color_id']==3]\n",
    "\n",
    "print(len(target))\n",
    "print(target.head())\n",
    "# Siendo estrictos con el enunciado, estos son los 34 productos que verifican tener color_id = 3 y pertenecer al menor código de familia con descuento (family = 1)\n",
    "\n",
    "target_2 = products_df[(products_df['discount']==1) & (products_df['family']==min_fam_desc) & (products_df['color_id']==3)]\n",
    "print(len(target_2))\n",
    "print(target_2)\n",
    "# Siendo menos estrictos, y entendiendo que la condición de tener descuento hay que aplicarla también sobre el resultado final, (discount=1), entonces tendríamos 2 productos:\n",
    "\n",
    "# No existe un resultado único para la pregunta descrita, por lo que escogemos como resultado el primer partnumber 18091 , que verifica ambas interpretaciones de la pregunta.\n",
    "Q1 = target_2.iloc[0]['partnumber']\n",
    "print(Q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Q2:** In the country where most users have made purchases totaling less than 500 (`M`) , which is the user who has the lowest purchase frequency (`F`), the most recent purchase (highest `R`) and the lowest `user_id`? Follow the given order of variables as the sorting priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country with most users having purchases <500: 25\n",
      "187374\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el total por usuaro de M, F y R,\n",
    "unique_users_df = users_df.groupby(['user_id', 'country']).agg({\n",
    "    'M': 'sum',         # Sum purchases per country\n",
    "    'F': 'sum',         # Sum frequency per country\n",
    "    'R': 'max'          # Most recent per country\n",
    "}).reset_index()\n",
    "\n",
    "# Obtenemos el país con mayor número de usuarios que tienen < 500 en la columna M (compras)\n",
    "unique_users_df['less_500_purchases'] = unique_users_df['M'] < 500\n",
    "country_agg = unique_users_df.groupby('country', as_index=False)['less_500_purchases'].sum()\n",
    "target_country = country_agg.loc[country_agg['less_500_purchases'].idxmax(), 'country']\n",
    "print(f'Country with most users having purchases <500: {target_country}')\n",
    "\n",
    "# Filtramos el país obtenido anteriormente y ordenamos los usuarios por F (menor a mayor), R (mayor a menor) y user_id (menor a mayor).\n",
    "users_df_c = unique_users_df[unique_users_df['country'] == target_country]\n",
    "sorted_user = users_df_c.sort_values(\n",
    "    by=['F', 'R', 'user_id'], \n",
    "    ascending=[True, False, True]\n",
    ").iloc[0]\n",
    "\n",
    "Q2 = sorted_user['user_id']\n",
    "print(Q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Q3:** Among the products that were added to the cart at least once, how many times is a product visited before it is added to the cart in average? Give the answer with 2 decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos los productos que han sido añadidos al carrito al menos una vez\n",
    "products_subset = train_df[train_df['add_to_cart']==True]['partnumber'].unique()\n",
    "train_df_s = train_df[train_df['partnumber'].isin(products_subset)]\n",
    "\n",
    "# Agrupamos y contamos registros (visitas o interacciones) y sumamos las veces que se ha añadido al carrito\n",
    "product_stats = train_df_s.groupby('partnumber').agg({\n",
    "    'add_to_cart': ['sum','count']\n",
    "}).reset_index()\n",
    "product_stats.columns = ['partnumber', 'cart_adds', 'total_visits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.69\n"
     ]
    }
   ],
   "source": [
    "# Cálculo\n",
    "product_stats['visits_before_cart'] = (product_stats['total_visits'] - product_stats['cart_adds']) / product_stats['cart_adds']\n",
    "\n",
    "Q3 = round(product_stats['visits_before_cart'].mean(),2) \n",
    "print(Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.71\n"
     ]
    }
   ],
   "source": [
    "# Get products added to cart and their first cart addition time\n",
    "cart_additions = train_df[train_df['add_to_cart'] == 1].groupby('partnumber')['timestamp_local'].min()\n",
    "\n",
    "# For these products, count visits up until first cart addition\n",
    "result = (\n",
    "    train_df.merge(\n",
    "        cart_additions.reset_index(),\n",
    "        on='partnumber',\n",
    "        suffixes=('', '_first_cart')\n",
    "    )\n",
    "    .query('timestamp_local <= timestamp_local_first_cart')\n",
    "    .groupby('partnumber')\n",
    "    .size()\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "Q3 = round(result, 2)\n",
    "print(Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Q4:** Which device (`device_type`) is most frequently used by users to make purchases (`add_to_cart` = 1) of discounted products (`discount` = 1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Filtramos los datasets por separado y luego resolvemos el inner join\n",
    "train_df_added_cart = train_df[train_df['add_to_cart']==True]\n",
    "discounted_products_df = products_df[products_df['discount'] == 1]\n",
    "result_df = train_df_added_cart.merge(discounted_products_df[['partnumber']], on='partnumber', how='inner')[['device_type']]\n",
    "\n",
    "# Obtenemos la moda (valor más frecuente) de la columna 'device_type' en el resultado\n",
    "Q4 = result_df['device_type'].mode()[0]\n",
    "print(Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Q5:** Among users with purchase frequency (`F`) in the top 3 within their purchase country, who has interacted with the most products (`partnumber`) in sessions conducted from a device with identifier 3 (`device_type` = 3)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chans\\AppData\\Local\\Temp\\ipykernel_19292\\878451378.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_3_per_country = users_df.groupby('country').apply(lambda x: x.nlargest(3, 'F')).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72153\n"
     ]
    }
   ],
   "source": [
    "# Obtener los 3 usuarios con mayor valor de F para cada valor de country\n",
    "top_3_per_country = users_df.groupby('country').apply(lambda x: x.nlargest(3, 'F')).reset_index(drop=True)\n",
    "\n",
    "# Filtramos los usuarios en el dataset de interacciones, y filtramos que sean sesiones en device_type = 3\n",
    "target_users = top_3_per_country['user_id'].unique()\n",
    "train_df_a = train_df[(train_df['user_id'].isin(target_users)) & (train_df['device_type'] == 3)]\n",
    "result_df = train_df_a.groupby('user_id').agg({\n",
    "    'partnumber' : 'nunique'\n",
    "}).sort_values('partnumber', ascending=False)\n",
    "\n",
    "# Obtenemos el user_id que ha interaccionado con más productos\n",
    "Q5 = int(result_df.index[0])  \n",
    "print(Q5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Q6:** For interactions that occurred outside the user's country of residence, how many unique family identifiers are there? Take into account any registered country for each user, as there may be more than one country per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6856730\n",
      "14257\n"
     ]
    }
   ],
   "source": [
    "# Hacemos Join con los usuarios y sus países de residencia y nos quedamos con las filas que no cruzan\n",
    "train_df_b = train_df[train_df['user_id'].notna()].merge(users_df[['user_id', 'country']], \n",
    "                                on=['user_id', 'country'], \n",
    "                                how='left',\n",
    "                                indicator=True)\n",
    "\n",
    "print(len(train_df_b))\n",
    "\n",
    "train_df_b = train_df_b[train_df_b['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "\n",
    "print(len(train_df_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_c = train_df_b.merge(products_df[['partnumber', 'family']], \n",
    "                                on=['partnumber'], \n",
    "                                how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "Q6 = train_df_c['family'].nunique() \n",
    "print(Q6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Q7:** Among interactions from the first 7 days of June, which is the most frequent page type where each family is added to the cart? Return it in the following format: `{'('family'): int('most_frequent_pagetype')}` . In case of a tie, return the smallest pagetype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_d = train_df[(train_df['date'] >= '2024-06-01') & (train_df['date'] <= '2024-06-07') & (train_df['add_to_cart'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 24, '2': 24, '3': 24, '4': 24, '5': 24, '6': 24, '7': 24, '8': 24, '9': 24, '10': 24, '11': 24, '12': 24, '13': 24, '14': 24, '15': 24, '16': 24, '17': 24, '18': 24, '19': 24, '21': 24, '22': 24, '23': 24, '24': 24, '26': 24, '27': 24, '28': 24, '29': 24, '30': 24, '31': 24, '32': 24, '33': 24, '34': 24, '35': 24, '36': 24, '37': 24, '38': 24, '40': 24, '41': 24, '42': 24, '43': 24, '44': 24, '45': 24, '46': 24, '47': 24, '48': 24, '49': 24, '50': 24, '51': 24, '52': 24, '53': 24, '54': 24, '56': 24, '57': 24, '58': 24, '59': 24, '60': 24, '61': 24, '62': 24, '63': 24, '64': 24, '65': 24, '66': 24, '67': 24, '68': 24, '69': 24, '70': 24, '71': 24, '72': 24, '73': 24, '74': 24, '75': 24, '76': 24, '77': 24, '78': 24, '79': 24, '81': 24, '82': 24, '83': 24, '84': 24, '85': 24, '86': 24, '87': 24, '88': 24, '89': 24, '90': 24, '91': 24, '92': 24, '93': 24, '94': 24, '95': 24, '96': 24, '97': 24, '99': 24, '100': 24, '101': 24, '102': 24, '103': 24, '104': 24, '105': 24, '106': 24, '107': 24, '108': 24, '109': 24, '110': 24, '111': 24, '112': 24, '113': 24, '114': 24, '115': 24, '116': 24, '117': 24, '118': 24, '119': 24, '120': 24, '121': 24, '122': 24, '123': 24, '124': 24, '125': 24, '126': 24, '127': 24, '128': 24, '129': 24, '130': 24, '131': 24, '132': 24, '133': 24, '134': 24, '135': 24, '136': 24, '137': 24, '138': 24, '139': 24, '140': 24, '141': 24, '142': 24, '143': 24, '144': 24, '145': 24, '146': 24, '147': 24, '148': 24, '149': 24, '150': 24, '151': 24, '152': 24, '153': 24, '154': 24, '155': 24, '156': 24, '157': 24, '158': 24, '159': 24, '160': 24, '161': 24, '162': 24, '163': 24, '164': 24, '165': 24, '166': 24, '167': 24, '168': 24, '169': 24, '170': 24, '171': 24, '172': 24, '173': 24, '174': 24, '175': 24, '177': 24, '179': 24, '180': 24, '181': 24, '182': 24, '183': 24, '184': 24, '185': 24, '186': 24, '187': 24, '188': 24, '189': 24, '190': 24, '191': 24, '192': 24, '193': 24, '194': 24, '195': 24, '196': 24, '197': 24, '199': 24, '200': 24, '201': 24, '202': 24, '203': 24, '205': 24, '206': 24, '207': 24, '208': 19, '209': 24, '210': 24, '211': 24, '214': 24}\n"
     ]
    }
   ],
   "source": [
    "train_df_e = train_df_d.merge(products_df[['partnumber', 'family']], \n",
    "    on=['partnumber'], \n",
    "    how='inner')\n",
    "\n",
    "# Agrupar por familia y tipo de pagina y contar las apariciones, ordenar descendente\n",
    "train_df_f = train_df_e.groupby(['family','pagetype']).size().reset_index(name='count')\n",
    "train_df_f = train_df_f.sort_values('count', ascending=False)\n",
    "\n",
    "# Crear el diccionaro objetivo\n",
    "Q7 = {}\n",
    "for family in sorted(train_df_f['family'].unique()):\n",
    "    family_data = train_df_f[train_df_f['family'] == family]\n",
    "    # Ordenar por apariciones descendente, y por pagetype ascendente (caso de empate)\n",
    "    most_common = family_data.sort_values(['count', 'pagetype'], ascending=[False, True]).iloc[0]\n",
    "    Q7[str(family)] = int(most_common['pagetype'])\n",
    "\n",
    "print(Q7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entregamos en el formato deseado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert numpy types to native Python types\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    return obj\n",
    "\n",
    "# Create predictions dictionary\n",
    "predictions = {\n",
    "    \"target\": {\n",
    "        \"query_1\": {\n",
    "            \"partnumber\": int(Q1)\n",
    "        },\n",
    "        \"query_2\": {\n",
    "            \"user_id\": int(Q2)\n",
    "        },\n",
    "        \"query_3\": {\n",
    "            \"average_previous_visits\": float(Q3)\n",
    "        },\n",
    "        \"query_4\": {\n",
    "            \"device_type\": int(Q4)\n",
    "        },\n",
    "        \"query_5\": {\n",
    "            \"user_id\": int(Q5)\n",
    "        },\n",
    "        \"query_6\": {\n",
    "            \"unique_families\": int(Q6)\n",
    "        },\n",
    "        \"query_7\": convert_to_serializable(Q7) \n",
    "    }\n",
    "}\n",
    "\n",
    "# Create predictions directory if it doesn't exist\n",
    "os.makedirs('predictions', exist_ok=True)\n",
    "\n",
    "# Save to JSON file with proper formatting\n",
    "with open('../../predictions/predictions_1.json', 'w') as f:\n",
    "    json.dump(predictions, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
