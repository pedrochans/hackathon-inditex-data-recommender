{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook para desarrollar la Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sistema de recomendación de 5 productos para cada sesión nueva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de los DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path    \n",
    "import pandas as pd\n",
    "\n",
    "RAW_DATA_PATH = Path('../../data/raw')\n",
    "\n",
    "train_df = pd.read_csv(         RAW_DATA_PATH / 'train.csv')\n",
    "test_df = pd.read_csv(          RAW_DATA_PATH / 'test.csv')\n",
    "products_df = pd.read_pickle(   RAW_DATA_PATH / 'products.pkl')\n",
    "users_df = pd.read_csv(         RAW_DATA_PATH / 'users_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer datos del string (JSON) de la columna 'values'\n",
    "def extract_data_from_string(df, column_name):\n",
    "    df['country'] = df[column_name].str.extract(r\"'country': \\[(.*?)\\]\").map(lambda x: int(x.split(', ')[0]))\n",
    "    df['R'] = df[column_name].str.extract(r\"'R': \\[(.*?)\\]\").map(lambda x: int(x.split(', ')[0]))\n",
    "    df['F'] = df[column_name].str.extract(r\"'F': \\[(.*?)\\]\").map(lambda x: int(x.split(', ')[0]))\n",
    "    df['M'] = df[column_name].str.extract(r\"'M': \\[(.*?)\\]\").map(lambda x: float(x.split(', ')[0]))\n",
    "    \n",
    "    return df.drop(columns=[column_name])\n",
    "\n",
    "users_df = extract_data_from_string(users_df, 'values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users DataFrame dtypes:\n",
      " user_id      int64\n",
      "country      int64\n",
      "R            int64\n",
      "F            int64\n",
      "M          float64\n",
      "dtype: object\n",
      "\n",
      "Train DataFrame dtypes:\n",
      " session_id                  int64\n",
      "date                       object\n",
      "timestamp_local    datetime64[ns]\n",
      "add_to_cart                 int64\n",
      "user_id                     int32\n",
      "country                     int64\n",
      "partnumber                  int64\n",
      "device_type                 int64\n",
      "pagetype                    int32\n",
      "dtype: object\n",
      "\n",
      "Test DataFrame dtypes:\n",
      " session_id                  int64\n",
      "date                       object\n",
      "timestamp_local    datetime64[ns]\n",
      "user_id                     int32\n",
      "country                     int64\n",
      "partnumber                  int64\n",
      "device_type                 int64\n",
      "pagetype                    int32\n",
      "dtype: object\n",
      "\n",
      "Products DataFrame dtypes:\n",
      " discount        int32\n",
      "embedding      object\n",
      "partnumber      int32\n",
      "color_id        int32\n",
      "cod_section     int32\n",
      "family          int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Products DataFrame Processing\n",
    "products_df['discount'] = products_df['discount'].astype(int)\n",
    "products_df['cod_section'] = products_df['cod_section'].fillna(products_df['cod_section'].median()).astype(int)\n",
    "\n",
    "# Train DataFrame Processing\n",
    "train_df['user_id'] = train_df['user_id'].fillna(0).astype(int)\n",
    "train_df['timestamp_local'] = pd.to_datetime(train_df['timestamp_local'])\n",
    "train_df['pagetype'] = train_df['pagetype'].fillna(train_df['pagetype'].mode()[0]).astype(int)\n",
    "\n",
    "test_df['user_id'] = test_df['user_id'].fillna(0).astype(int)\n",
    "test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local'])\n",
    "test_df['pagetype'] = test_df['pagetype'].fillna(test_df['pagetype'].mode()[0]).astype(int)\n",
    "\n",
    "# Verify conversions\n",
    "print(\"Users DataFrame dtypes:\\n\", users_df.dtypes)\n",
    "print(\"\\nTrain DataFrame dtypes:\\n\", train_df.dtypes)\n",
    "print(\"\\nTest DataFrame dtypes:\\n\", test_df.dtypes)\n",
    "print(\"\\nProducts DataFrame dtypes:\\n\", products_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users DataFrame dtypes:\n",
      " user_id       int64\n",
      "country    category\n",
      "R             int64\n",
      "F             int64\n",
      "M           float64\n",
      "dtype: object\n",
      "\n",
      "Train DataFrame dtypes:\n",
      " session_id                  int64\n",
      "date                       object\n",
      "timestamp_local    datetime64[ns]\n",
      "add_to_cart                 int64\n",
      "user_id                     int32\n",
      "country                  category\n",
      "partnumber                  int64\n",
      "device_type              category\n",
      "pagetype                 category\n",
      "dtype: object\n",
      "\n",
      "Test DataFrame dtypes:\n",
      " session_id                  int64\n",
      "date                       object\n",
      "timestamp_local    datetime64[ns]\n",
      "user_id                     int32\n",
      "country                  category\n",
      "partnumber                  int64\n",
      "device_type              category\n",
      "pagetype                 category\n",
      "dtype: object\n",
      "\n",
      "Products DataFrame dtypes:\n",
      " discount          int32\n",
      "embedding        object\n",
      "partnumber        int32\n",
      "color_id       category\n",
      "cod_section    category\n",
      "family         category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convertimos las variables a categoricas, nos conviene para que el modelo las interprete como tal\n",
    "users_df['country'] = users_df['country'].astype('category')\n",
    "\n",
    "categorical_cols_train = ['device_type', 'pagetype', 'country']\n",
    "for col in categorical_cols_train:\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].astype('category')\n",
    "\n",
    "categorical_cols_products = ['color_id', 'cod_section', 'family']\n",
    "for col in categorical_cols_products:\n",
    "    products_df[col] = products_df[col].astype('category')\n",
    "\n",
    "print(\"Users DataFrame dtypes:\\n\", users_df.dtypes)\n",
    "print(\"\\nTrain DataFrame dtypes:\\n\", train_df.dtypes)\n",
    "print(\"\\nTest DataFrame dtypes:\\n\", test_df.dtypes)\n",
    "print(\"\\nProducts DataFrame dtypes:\\n\", products_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  user_id\n",
      "0         641      641\n",
      "1        1171     1171\n",
      "2        1171     1171\n",
      "3        5791     5791\n",
      "4       12201  4807290\n",
      "   session_id  user_id\n",
      "0        7461     7461\n",
      "1        7461     7461\n",
      "2        7461     7461\n",
      "3        7461     7461\n",
      "4        7461     7461\n",
      "0    1803480\n",
      "1    1754230\n",
      "2    1803490\n",
      "3    1803500\n",
      "4    1754670\n",
      "Name: user_id, dtype: int64\n",
      "\n",
      "Existen colisiones: False\n"
     ]
    }
   ],
   "source": [
    "# El user_id trae muchos valores nulos, por lo que se nos ocurre crear dos nuevos ids user_id, session_id sin colisiones entre ellos.\n",
    "# El objetivo es reeplazar los nulos en user_id por el session_id, de forma que si no tenemos el dato de usuario, al menos\n",
    "#     somos capaces de relacionar registros por sesión (supongamos, por ejemplo que son usuarios no registrados en la web)\n",
    "\n",
    "# Usamos columnas temporales para crear los nuevos\n",
    "users_df['temp_user_id'] = users_df['user_id'] * 10\n",
    "train_df['temp_session_id'] = train_df['session_id'] * 10 + 1\n",
    "train_df['temp_user_id'] = train_df['user_id'] * 10\n",
    "test_df['temp_session_id'] = test_df['session_id'] * 10 + 1\n",
    "test_df['temp_user_id'] = test_df['user_id'] * 10\n",
    "\n",
    "train_df.loc[train_df['temp_user_id'] == 0, 'temp_user_id'] = train_df.loc[train_df['temp_user_id'] == 0, 'temp_session_id'].astype('int32')\n",
    "test_df.loc[test_df['temp_user_id'] == 0, 'temp_user_id'] = test_df.loc[test_df['temp_user_id'] == 0, 'temp_session_id'].astype('int32')\n",
    "\n",
    "users_df['user_id'] = users_df['temp_user_id']\n",
    "train_df['session_id'] = train_df['temp_session_id']\n",
    "train_df['user_id'] = train_df['temp_user_id']\n",
    "test_df['session_id'] = test_df['temp_session_id']\n",
    "test_df['user_id'] = test_df['temp_user_id']\n",
    "\n",
    "users_df.drop('temp_user_id', axis=1, inplace=True)\n",
    "train_df.drop(['temp_session_id', 'temp_user_id'], axis=1, inplace=True)\n",
    "test_df.drop(['temp_session_id', 'temp_user_id'], axis=1, inplace=True)\n",
    "\n",
    "# Print, y verificar que no hay colisiones entre los ids.\n",
    "print(train_df[['session_id', 'user_id']].head())\n",
    "print(test_df[['session_id', 'user_id']].head())\n",
    "print(users_df['user_id'].head())\n",
    "\n",
    "print(\"\\nExisten colisiones:\", \n",
    "      bool(set(users_df['user_id']).intersection(set(train_df['session_id']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Añadimos variables de interés calculadas a nivel de user_id\"\"\"\n",
    "    \n",
    "    print(\"Transformamos timestamps...\")\n",
    "    df['total_seconds'] = df['timestamp_local'].astype(np.int64) // 1e9\n",
    "    df['hour'] = df['timestamp_local'].dt.hour\n",
    "    \n",
    "    print(\"Agrupamos por user_id...\")\n",
    "    grouped = df.groupby('user_id', observed=True).agg({\n",
    "        'partnumber': 'nunique',\n",
    "        'pagetype': 'nunique',\n",
    "        'total_seconds': ['min', 'max'],\n",
    "        'hour': 'first',\n",
    "        'user_id': 'size' # Interacciones\n",
    "    }).reset_index()\n",
    "    \n",
    "    grouped.columns = ['user_id', 'unique_products', 'unique_pagetypes', \n",
    "                      'min_time', 'max_time', 'first_interaction_hour',\n",
    "                      'total_interactions']\n",
    "    \n",
    "    # Calculamos el tiempo\n",
    "    grouped['total_user_time'] = (grouped['max_time'] - grouped['min_time']).round(2)\n",
    "    grouped = grouped.drop(['min_time', 'max_time'], axis=1)\n",
    "    \n",
    "    # Merge\n",
    "    print(\"Merging results...\")\n",
    "    result = df.merge(grouped, on='user_id')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformamos timestamps...\n",
      "Agrupamos por user_id...\n",
      "Merging results...\n",
      "Transformamos timestamps...\n",
      "Agrupamos por user_id...\n",
      "Merging results...\n"
     ]
    }
   ],
   "source": [
    "train_df = add_user_features(train_df)\n",
    "test_df = add_user_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadiremos tres columnas, el ratio de exito del usuario con el producto, el ratio de exito global del producto, y el numero de veces que se ha añadido al carrito globalmente cada producto.\n",
    "user_product_metrics = (\n",
    "    train_df\n",
    "    .groupby(['user_id', 'partnumber'])\n",
    "    .agg({\n",
    "        'add_to_cart': ['sum', 'count']\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "user_product_metrics.columns = ['user_id', 'partnumber', 'adds_to_cart', 'total_visits']\n",
    "user_product_metrics['success_rate'] = user_product_metrics['adds_to_cart'] / user_product_metrics['total_visits']\n",
    "\n",
    "train_df = train_df.merge(\n",
    "    user_product_metrics[['user_id', 'partnumber', 'success_rate']], \n",
    "    on=['user_id', 'partnumber'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "product_metrics = (\n",
    "    train_df\n",
    "    .groupby('partnumber')\n",
    "    .agg({\n",
    "        'add_to_cart': ['sum', 'count']\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "product_metrics.columns = ['partnumber', 'total_adds_to_cart', 'total_visits']\n",
    "product_metrics['global_success_rate'] = product_metrics['total_adds_to_cart'] / product_metrics['total_visits']\n",
    "\n",
    "products_df = products_df.merge(\n",
    "    product_metrics[['partnumber', 'total_adds_to_cart', 'global_success_rate']], \n",
    "    on='partnumber', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Cubrimos nulos\n",
    "train_df['success_rate'] = train_df['success_rate'].fillna(0)\n",
    "products_df['total_adds_to_cart'] = products_df['total_adds_to_cart'].fillna(0)\n",
    "products_df['global_success_rate'] = products_df['global_success_rate'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_dataframes(dfs_dict):\n",
    "    for name, df in dfs_dict.items():\n",
    "        print(f\"\\n{'='*20} {name} Analysis {'='*20}\")\n",
    "        \n",
    "        print(f\"\\nShape: {df.shape}\")\n",
    "        print(f\"Memory Usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        print(\"\\nColumns and Types:\")\n",
    "        print(df.dtypes)\n",
    "        \n",
    "        print(\"\\nMissing Values:\")\n",
    "        print(df.isnull().sum())\n",
    "\n",
    "        print(\"\\nNumerical Columns Statistics:\")\n",
    "        print(df.describe())\n",
    "        \n",
    "        print(\"\\nSample Data:\")\n",
    "        print(df.head())\n",
    "\n",
    "dfs = {\n",
    "    'Train': train_df,\n",
    "    'Test': test_df,\n",
    "    'Users': users_df,\n",
    "    'Products': products_df\n",
    "}\n",
    "\n",
    "analyze_dataframes(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.283918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1877\n",
      "[LightGBM] [Info] Number of data points in the train set: 46551445, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 0.058984\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def prepare_features(df, users_df, products_df):\n",
    "    \n",
    "    # Temporales\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local']).dt.hour.astype('int8')\n",
    "    df['day'] = pd.to_datetime(df['timestamp_local']).dt.day.astype('int8')\n",
    "    \n",
    "    # Merge de los datasets\n",
    "    df = df.merge(users_df[['user_id', 'R', 'F', 'M']], on='user_id', how='left')\n",
    "    df = df.merge(products_df[['partnumber', 'discount']], on='partnumber', how='left')\n",
    "    \n",
    "\n",
    "    features = ['hour', 'day', 'country', 'device_type', 'pagetype',\n",
    "                'R', 'F', 'M', 'discount', 'total_seconds',\n",
    "                'unique_products', 'unique_pagetypes', 'first_interaction_hour', 'total_interactions', 'total_user_time']\n",
    "    \n",
    "    return df[features]\n",
    "\n",
    "# Prepare train and test data\n",
    "X_train = prepare_features(train_df, users_df, products_df)\n",
    "y_train = train_df['success_rate']\n",
    "X_test = prepare_features(test_df, users_df, products_df)\n",
    "\n",
    "print('Entrenando modelo...')\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediccion\n",
    "test_df['predicted_success_rate'] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completaremos las recomendaciones con productos populares\n",
    "popular_products = (\n",
    "    products_df\n",
    "    .sort_values('global_success_rate', ascending=False)\n",
    "    ['partnumber']\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "enriched_test = (\n",
    "    test_df\n",
    "    .merge(\n",
    "        products_df[['partnumber', 'global_success_rate']], \n",
    "        on='partnumber', \n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Crearemos una composición para las recomendaciones, ponderando popularidad global y predicción\n",
    "PREDICTION_WEIGHT = 0.8\n",
    "GLOBAL_WEIGHT = 0.2\n",
    "\n",
    "enriched_test['composite_score'] = (\n",
    "    PREDICTION_WEIGHT * enriched_test['predicted_success_rate'] + \n",
    "    GLOBAL_WEIGHT * enriched_test['global_success_rate'].fillna(0)\n",
    ")\n",
    "\n",
    "recommendations = (\n",
    "    enriched_test\n",
    "    .sort_values(['composite_score'], ascending=[False])\n",
    "    .groupby('session_id')\n",
    "    .agg({\n",
    "        'partnumber': lambda x: list(x)\n",
    "    })\n",
    ")\n",
    "\n",
    "def pad_recommendations(prods):\n",
    "    if len(prods) >= 5:\n",
    "        return prods[:5]\n",
    "    return prods + [p for p in popular_products if p not in prods][:5-len(prods)]\n",
    "\n",
    "result_dict = {\n",
    "    \"target\": {\n",
    "        str(session_id)[:-1]: pad_recommendations(prods)\n",
    "        for session_id, prods in recommendations['partnumber'].items()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardamos las predicciones en un archivo JSON\n",
    "output_path = Path('../../predictions/predictions_3.json')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(result_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
