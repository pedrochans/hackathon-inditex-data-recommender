{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook para desarrollar la Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sistema de recomendación de 5 productos para cada sesión nueva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de los DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path    \n",
    "import pandas as pd\n",
    "\n",
    "RAW_DATA_PATH = Path('../../data/raw')\n",
    "\n",
    "train_df = pd.read_csv(         RAW_DATA_PATH / 'train.csv')\n",
    "test_df = pd.read_csv(          RAW_DATA_PATH / 'test.csv')\n",
    "products_df = pd.read_pickle(   RAW_DATA_PATH / 'products.pkl')\n",
    "users_df = pd.read_csv(         RAW_DATA_PATH / 'users_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Extraer datos del string (JSON) de la columna 'values'\n",
    "def extract_data_from_string(df, column_name):\n",
    "    \n",
    "    df[column_name] = df[column_name].apply(ast.literal_eval)\n",
    "    \n",
    "    df['country'] = df[column_name].apply(lambda x: x['country'])\n",
    "    df['R'] = df[column_name].apply(lambda x: x['R'])\n",
    "    df['F'] = df[column_name].apply(lambda x: x['F'])\n",
    "    df['M'] = df[column_name].apply(lambda x: x['M'])\n",
    "    \n",
    "    df = df.drop(columns=[column_name])\n",
    "    \n",
    "    df = df.explode(['country', 'R', 'F', 'M'])\n",
    "    \n",
    "    df['country'] = df['country'].astype(int)\n",
    "    df['R'] = df['R'].astype(int)\n",
    "    df['F'] = df['F'].astype(int)\n",
    "    df['M'] = df['M'].astype(float)\n",
    "    \n",
    "    return df\n",
    "\n",
    "users_df = extract_data_from_string(users_df, 'values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users DataFrame dtypes:\n",
      " user_id      int64\n",
      "country      int32\n",
      "R            int32\n",
      "F            int32\n",
      "M          float64\n",
      "dtype: object\n",
      "\n",
      "Train DataFrame dtypes:\n",
      " session_id                  int64\n",
      "date                       object\n",
      "timestamp_local    datetime64[ns]\n",
      "add_to_cart                 int64\n",
      "user_id                     int32\n",
      "country                     int64\n",
      "partnumber                  int64\n",
      "device_type                 int64\n",
      "pagetype                    int32\n",
      "dtype: object\n",
      "\n",
      "Test DataFrame dtypes:\n",
      " session_id                  int64\n",
      "date                       object\n",
      "timestamp_local    datetime64[ns]\n",
      "user_id                     int32\n",
      "country                     int64\n",
      "partnumber                  int64\n",
      "device_type                 int64\n",
      "pagetype                    int32\n",
      "dtype: object\n",
      "\n",
      "Products DataFrame dtypes:\n",
      " discount        int32\n",
      "embedding      object\n",
      "partnumber      int32\n",
      "color_id        int32\n",
      "cod_section     int32\n",
      "family          int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Products DataFrame Processing\n",
    "products_df['discount'] = products_df['discount'].astype(int)\n",
    "products_df['cod_section'] = products_df['cod_section'].fillna(products_df['cod_section'].median()).astype(int)\n",
    "\n",
    "# Train DataFrame Processing\n",
    "train_df['user_id'] = train_df['user_id'].fillna(0).astype(int)\n",
    "train_df['timestamp_local'] = pd.to_datetime(train_df['timestamp_local'])\n",
    "train_df['pagetype'] = train_df['pagetype'].fillna(train_df['pagetype'].mode()[0]).astype(int)\n",
    "\n",
    "test_df['user_id'] = test_df['user_id'].fillna(0).astype(int)\n",
    "test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local'])\n",
    "test_df['pagetype'] = test_df['pagetype'].fillna(test_df['pagetype'].mode()[0]).astype(int)\n",
    "\n",
    "# Verify conversions\n",
    "print(\"Users DataFrame dtypes:\\n\", users_df.dtypes)\n",
    "print(\"\\nTrain DataFrame dtypes:\\n\", train_df.dtypes)\n",
    "print(\"\\nTest DataFrame dtypes:\\n\", test_df.dtypes)\n",
    "print(\"\\nProducts DataFrame dtypes:\\n\", products_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users DataFrame dtypes:\n",
      " user_id       int64\n",
      "country    category\n",
      "R             int32\n",
      "F             int32\n",
      "M           float64\n",
      "dtype: object\n",
      "\n",
      "Train DataFrame dtypes:\n",
      " session_id                  int64\n",
      "date                       object\n",
      "timestamp_local    datetime64[ns]\n",
      "add_to_cart                 int64\n",
      "user_id                     int32\n",
      "country                  category\n",
      "partnumber                  int64\n",
      "device_type              category\n",
      "pagetype                 category\n",
      "dtype: object\n",
      "\n",
      "Test DataFrame dtypes:\n",
      " session_id                  int64\n",
      "date                       object\n",
      "timestamp_local    datetime64[ns]\n",
      "user_id                     int32\n",
      "country                  category\n",
      "partnumber                  int64\n",
      "device_type              category\n",
      "pagetype                 category\n",
      "dtype: object\n",
      "\n",
      "Products DataFrame dtypes:\n",
      " discount          int32\n",
      "embedding        object\n",
      "partnumber        int32\n",
      "color_id       category\n",
      "cod_section    category\n",
      "family         category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convertimos las variables a categoricas, nos conviene para que el modelo las interprete como tal\n",
    "users_df['country'] = users_df['country'].astype('category')\n",
    "\n",
    "categorical_cols_train = ['device_type', 'pagetype', 'country']\n",
    "for col in categorical_cols_train:\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].astype('category')\n",
    "\n",
    "categorical_cols_products = ['color_id', 'cod_section', 'family']\n",
    "for col in categorical_cols_products:\n",
    "    products_df[col] = products_df[col].astype('category')\n",
    "\n",
    "print(\"Users DataFrame dtypes:\\n\", users_df.dtypes)\n",
    "print(\"\\nTrain DataFrame dtypes:\\n\", train_df.dtypes)\n",
    "print(\"\\nTest DataFrame dtypes:\\n\", test_df.dtypes)\n",
    "print(\"\\nProducts DataFrame dtypes:\\n\", products_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  user_id\n",
      "0         641      641\n",
      "1        1171     1171\n",
      "2        1171     1171\n",
      "3        5791     5791\n",
      "4       12201  4807290\n",
      "   session_id  user_id\n",
      "0        7461     7461\n",
      "1        7461     7461\n",
      "2        7461     7461\n",
      "3        7461     7461\n",
      "4        7461     7461\n",
      "0    1803480\n",
      "1    1754230\n",
      "1    1754230\n",
      "2    1803490\n",
      "3    1803500\n",
      "Name: user_id, dtype: int64\n",
      "\n",
      "Existen colisiones: False\n"
     ]
    }
   ],
   "source": [
    "# El user_id trae muchos valores nulos, por lo que se nos ocurre crear dos nuevos ids user_id, session_id sin colisiones entre ellos.\n",
    "# El objetivo es reeplazar los nulos en user_id por el session_id, de forma que si no tenemos el dato de usuario, al menos\n",
    "#     somos capaces de relacionar registros por sesión (supongamos, por ejemplo que son usuarios no registrados en la web)\n",
    "\n",
    "# Usamos columnas temporales para crear los nuevos\n",
    "users_df['temp_user_id'] = users_df['user_id'] * 10\n",
    "train_df['temp_session_id'] = train_df['session_id'] * 10 + 1\n",
    "train_df['temp_user_id'] = train_df['user_id'] * 10\n",
    "test_df['temp_session_id'] = test_df['session_id'] * 10 + 1\n",
    "test_df['temp_user_id'] = test_df['user_id'] * 10\n",
    "\n",
    "train_df.loc[train_df['temp_user_id'] == 0, 'temp_user_id'] = train_df.loc[train_df['temp_user_id'] == 0, 'temp_session_id'].astype('int32')\n",
    "test_df.loc[test_df['temp_user_id'] == 0, 'temp_user_id'] = test_df.loc[test_df['temp_user_id'] == 0, 'temp_session_id'].astype('int32')\n",
    "\n",
    "users_df['user_id'] = users_df['temp_user_id']\n",
    "train_df['session_id'] = train_df['temp_session_id']\n",
    "train_df['user_id'] = train_df['temp_user_id']\n",
    "test_df['session_id'] = test_df['temp_session_id']\n",
    "test_df['user_id'] = test_df['temp_user_id']\n",
    "\n",
    "users_df.drop('temp_user_id', axis=1, inplace=True)\n",
    "train_df.drop(['temp_session_id', 'temp_user_id'], axis=1, inplace=True)\n",
    "test_df.drop(['temp_session_id', 'temp_user_id'], axis=1, inplace=True)\n",
    "\n",
    "# Print, y verificar que no hay colisiones entre los ids.\n",
    "print(train_df[['session_id', 'user_id']].head())\n",
    "print(test_df[['session_id', 'user_id']].head())\n",
    "print(users_df['user_id'].head())\n",
    "\n",
    "print(\"\\nExisten colisiones:\", \n",
    "      bool(set(users_df['user_id']).intersection(set(train_df['session_id']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Añadimos variables de interés calculadas a nivel de user_id\"\"\"\n",
    "    \n",
    "    print(\"Transformamos timestamps...\")\n",
    "    df['total_seconds'] = df['timestamp_local'].astype(np.int64) // 1e9\n",
    "    df['total_seconds'] = df['total_seconds'].astype(int)\n",
    "    df['hour'] = df['timestamp_local'].dt.hour\n",
    "    \n",
    "    print(\"Agrupamos por user_id...\")\n",
    "    grouped = df.groupby('user_id', observed=True).agg({\n",
    "        'partnumber': 'nunique',\n",
    "        'pagetype': 'nunique',\n",
    "        'total_seconds': ['min', 'max'],\n",
    "        'hour': 'first',\n",
    "        'user_id': 'size' # Interacciones\n",
    "    }).reset_index()\n",
    "    \n",
    "    grouped.columns = ['user_id', 'unique_products', 'unique_pagetypes', \n",
    "                      'min_time', 'max_time', 'first_interaction_hour',\n",
    "                      'total_interactions']\n",
    "    \n",
    "    # Calculamos el tiempo\n",
    "    grouped['total_user_time'] = (grouped['max_time'] - grouped['min_time']).round(2).astype(np.int64)\n",
    "    grouped = grouped.drop(['min_time', 'max_time'], axis=1)\n",
    "    \n",
    "    # Merge\n",
    "    print(\"Merging results...\")\n",
    "    result = df.merge(grouped, on='user_id')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformamos timestamps...\n",
      "Agrupamos por user_id...\n",
      "Merging results...\n",
      "Transformamos timestamps...\n",
      "Agrupamos por user_id...\n",
      "Merging results...\n"
     ]
    }
   ],
   "source": [
    "train_df = add_user_features(train_df)\n",
    "test_df = add_user_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadiremos tres columnas, el ratio de exito del usuario con el producto, el ratio de exito global del producto, y el numero de veces que se ha añadido al carrito globalmente cada producto.\n",
    "user_product_metrics = (\n",
    "    train_df\n",
    "    .groupby(['user_id', 'partnumber'])\n",
    "    .agg({\n",
    "        'add_to_cart': ['sum', 'count']\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "user_product_metrics.columns = ['user_id', 'partnumber', 'adds_to_cart', 'total_visits']\n",
    "user_product_metrics['success_rate'] = user_product_metrics['adds_to_cart'] / user_product_metrics['total_visits']\n",
    "\n",
    "train_df = train_df.merge(\n",
    "    user_product_metrics[['user_id', 'partnumber', 'success_rate']], \n",
    "    on=['user_id', 'partnumber'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "product_metrics = (\n",
    "    train_df\n",
    "    .groupby('partnumber')\n",
    "    .agg({\n",
    "        'add_to_cart': ['sum', 'count']\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "product_metrics.columns = ['partnumber', 'total_adds_to_cart', 'total_visits']\n",
    "product_metrics['global_success_rate'] = product_metrics['total_adds_to_cart'] / product_metrics['total_visits']\n",
    "\n",
    "products_df = products_df.merge(\n",
    "    product_metrics[['partnumber', 'total_adds_to_cart', 'global_success_rate', 'total_visits']], \n",
    "    on='partnumber', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Cubrimos nulos\n",
    "train_df['success_rate'] = train_df['success_rate'].fillna(0)\n",
    "products_df['total_adds_to_cart'] = products_df['total_adds_to_cart'].fillna(0).astype(int)\n",
    "products_df['global_success_rate'] = products_df['global_success_rate'].fillna(0)\n",
    "products_df['total_visits'] = products_df['total_visits'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadimos el historial de usuario\n",
    "user_history = (\n",
    "    train_df\n",
    "    .groupby('user_id')\n",
    "    .agg(\n",
    "        avg_success_rate_per_user=('success_rate', 'mean'),\n",
    "        num_unique_products_interacted=('partnumber', 'nunique')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "users_df = users_df.merge(\n",
    "    user_history, \n",
    "    on='user_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Cubrimos nulos\n",
    "users_df['avg_success_rate_per_user'] = users_df['avg_success_rate_per_user'].fillna(0)\n",
    "users_df['num_unique_products_interacted'] = users_df['num_unique_products_interacted'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filtramos solo compras reales\n",
    "train_purchases = train_df[train_df[\"add_to_cart\"] == 1]\n",
    "\n",
    "# Contamos cuántas veces ha comprado cada usuario un producto antes de cada fila\n",
    "train_df[\"user_has_bought_product_before\"] = (\n",
    "    train_purchases.groupby([\"user_id\", \"partnumber\"]).cumcount()\n",
    "    .astype(\"int8\")\n",
    ")\n",
    "\n",
    "# Convertimos a 0/1: Si hay al menos una compra previa → 1, si no → 0\n",
    "train_df[\"user_has_bought_product_before\"] = (train_df[\"user_has_bought_product_before\"] > 0).astype(\"int8\").astype('category')\n",
    "\n",
    "# Creamos un set de pares (usuario, producto) que han comprado en train_df\n",
    "user_product_bought = set(zip(train_purchases[\"user_id\"], train_purchases[\"partnumber\"]))\n",
    "\n",
    "# Aplicamos la marca en test_df\n",
    "test_df[\"user_has_bought_product_before\"] = test_df.apply(\n",
    "    lambda row: 1 if (row[\"user_id\"], row[\"partnumber\"]) in user_product_bought else 0,\n",
    "    axis=1\n",
    ").astype(\"int8\").astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataframes(dfs_dict):\n",
    "    for name, df in dfs_dict.items():\n",
    "        print(f\"\\n{'='*20} {name} Analysis {'='*20}\")\n",
    "        \n",
    "        print(f\"\\nShape: {df.shape}\")\n",
    "        print(f\"Memory Usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        print(\"\\nColumns and Types:\")\n",
    "        print(df.dtypes)\n",
    "        \n",
    "        print(\"\\nMissing Values:\")\n",
    "        print(df.isnull().sum())\n",
    "\n",
    "        print(\"\\nNumerical Columns Statistics:\")\n",
    "        print(df.describe())\n",
    "        \n",
    "        print(\"\\nSample Data:\")\n",
    "        print(df.head())\n",
    "\n",
    "dfs = {\n",
    "    'Train': train_df,\n",
    "    'Test': test_df,\n",
    "    'Users': users_df,\n",
    "    'Products': products_df\n",
    "}\n",
    "\n",
    "analyze_dataframes(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo: Versión 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_features(df, users_df, products_df):\n",
    "    \n",
    "    # Temporales\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local']).dt.hour.astype('int8')\n",
    "\n",
    "    # Merge de los datasets\n",
    "    df = df.merge(users_df[['user_id', 'country', \n",
    "                            'R', 'F', 'M', \n",
    "                            'avg_success_rate_per_user', 'num_unique_products_interacted']], on=['user_id','country'], how='left')\n",
    "    df = df.merge(products_df[['partnumber', 'discount', 'family', 'color_id', 'cod_section', 'total_adds_to_cart', 'global_success_rate', 'total_visits']], on='partnumber', how='left')\n",
    "\n",
    "    features = ['hour', 'country', 'device_type', 'pagetype', \n",
    "                'family', 'color_id', 'cod_section', 'total_adds_to_cart', 'global_success_rate', 'total_visits', 'avg_success_rate_per_user', 'num_unique_products_interacted',\n",
    "                'R', 'F', 'M', 'discount', 'total_seconds', \n",
    "                'unique_products', 'unique_pagetypes', 'total_interactions', 'total_user_time',\n",
    "                'user_has_bought_product_before']\n",
    "\n",
    "    return df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test data\n",
    "X_train = prepare_features(train_df, users_df, products_df)\n",
    "y_train = train_df['success_rate']\n",
    "y_train_add_to_cart = train_df['add_to_cart']\n",
    "X_test = prepare_features(test_df, users_df, products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['num_unique_products_interacted'] = X_train['num_unique_products_interacted'].fillna(0).astype(int)\n",
    "X_test['num_unique_products_interacted'] = X_test['num_unique_products_interacted'].fillna(0).astype(int)\n",
    "\n",
    "X_train['F'] = X_train['F'].fillna(0).astype(int)\n",
    "X_test['F'] = X_test['F'].fillna(0).astype(int)\n",
    "\n",
    "X_train['R'] = X_train['R'].fillna(0).astype(int)\n",
    "X_test['R'] = X_test['R'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener un resumen del DataFrame\n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediccion\n",
    "test_df['predicted_success_rate'] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versión 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Separar las columnas categóricas para LightGBM\n",
    "categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Convertir las variables categóricas a códigos numéricos\n",
    "for col in categorical_features:\n",
    "    X_train[col] = X_train[col].cat.codes\n",
    "    X_test[col] = X_test[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un conjunto de validación\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train_add_to_cart, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear datasets de LightGBM\n",
    "train_data = lgb.Dataset(X_train_sub, label=y_train_sub, categorical_feature=categorical_features)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=categorical_features, reference=train_data)\n",
    "\n",
    "pos_weight = (len(y_train_add_to_cart) - sum(y_train_add_to_cart)) / sum(y_train_add_to_cart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de parámetros del modelo\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 64,\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 300, # 1000\n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample': 0.8,\n",
    "    'class_weight': pos_weight,\n",
    "    'random_state': 42,\n",
    "    'verbose': 2\n",
    "}\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, val_data]\n",
    ")\n",
    "\n",
    "# Predicción en el conjunto de test\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Evaluación\n",
    "print(f\"AUC en validación: {roc_auc_score(y_val, model.predict(X_val)):.4f}\")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save_model('../../models/lightgbm_add_to_cart_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "model.save_model('../../models/lightgbm_add_to_cart_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Cargar el modelo guardado\n",
    "model = lgb.Booster(model_file='../../models/lightgbm_add_to_cart_model.txt')\n",
    "\n",
    "X_test = prepare_features(test_df, users_df, products_df)\n",
    "X_test['num_unique_products_interacted'] = X_test['num_unique_products_interacted'].fillna(0).astype(int)\n",
    "X_test['F'] = X_test['F'].fillna(0).astype(int)\n",
    "X_test['R'] = X_test['R'].fillna(0).astype(int)\n",
    "\n",
    "# Separar las columnas categóricas para LightGBM\n",
    "categorical_features = X_test.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Convertir las variables categóricas a códigos numéricos\n",
    "for col in categorical_features:\n",
    "    X_test[col] = X_test[col].cat.codes\n",
    "\n",
    "# Predicción en el conjunto de test\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediccion\n",
    "test_df['predicted_success_rate'] = y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999244  0.99997501 0.9999787  ... 0.99998538 0.99998429 0.99998699]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones y resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Obtener importancia de cada feature\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Mostrar las 10 features más importantes\n",
    "print(\"🔍 Top 10 Features más importantes:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Mostrar las 10 menos importantes\n",
    "print(\"\\n⚠️ Features con menor importancia:\")\n",
    "print(feature_importance.tail(10))\n",
    "\n",
    "# Graficar la importancia de las features\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance en LGBM')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de productos similares y productos populares globalmente para mejorar recomendaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_products = (\n",
    "    products_df\n",
    "    .sort_values('global_success_rate', ascending=False)\n",
    "    ['partnumber']\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Normalización de características numéricas\n",
    "num_features = products_df[['discount', 'total_adds_to_cart', 'global_success_rate', 'total_visits']]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(num_features)\n",
    "\n",
    "# Codificación de características categóricas\n",
    "cat_features = products_df[['color_id', 'cod_section', 'family']]\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_features = encoder.fit_transform(cat_features)\n",
    "\n",
    "# Convertimos embeddings de productos en una matriz\n",
    "products_df['embedding'] = products_df['embedding'].apply(lambda x: x if x is not None else np.zeros(1280))\n",
    "\n",
    "# Combinamos todas las características\n",
    "combined_features = np.hstack([scaled_features, encoded_features, np.vstack(products_df['embedding'].values)])\n",
    "\n",
    "# Calculamos la similitud\n",
    "similarity_matrix = cosine_similarity(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamos cada producto con sus 5 más similares\n",
    "product_indices = {p: i for i, p in enumerate(products_df['partnumber'])}\n",
    "product_similarities = {}\n",
    "\n",
    "for partnumber, idx in product_indices.items():\n",
    "    similar_indices = np.argsort(-similarity_matrix[idx])[:6]  # Tomar los 6 productos más similares (incluyendo el mismo)\n",
    "    product_similarities[partnumber] = [ \n",
    "        products_df.iloc[i]['partnumber']\n",
    "        for i in similar_indices\n",
    "        if i != idx  # Evitar el mismo producto\n",
    "    ][:5]  # Tomar solo los 5 más similares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion de recomendacion y resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para obtener productos similares\n",
    "def get_similar_products(partnumber, k=3):\n",
    "    return list(dict.fromkeys(product_similarities.get(partnumber, [])))[:k]\n",
    "\n",
    "# Función de padding para asegurar 5 recomendaciones únicas\n",
    "def pad_recommendations(prods):\n",
    "    prods = list(dict.fromkeys(prods))  # Valores únicos\n",
    "    \n",
    "    if len(prods) >= 5:\n",
    "        return prods[:5]\n",
    "\n",
    "    # Añadimos productos similares si faltan recomendaciones\n",
    "    similar_prods = []\n",
    "    for p in prods:\n",
    "        similar_prods.extend(get_similar_products(p, k=3))\n",
    "\n",
    "    combined_prods = list(dict.fromkeys(prods + similar_prods))  # Valores únicos\n",
    "    \n",
    "    if len(combined_prods) >= 5:\n",
    "        return combined_prods[:5]\n",
    "\n",
    "    # Rellenamos con productos populares si aún faltan recomendaciones\n",
    "    remaining = [p for p in popular_products if p not in combined_prods]\n",
    "    final_recommendations = (combined_prods + remaining)[:5]\n",
    "\n",
    "    return final_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear listado de recomendaciones a partir de todos nuestros origenes, con 5 recomendaciones por sesión, y guardar en JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enriquecemos test_df con global_success_rate\n",
    "enriched_test = (\n",
    "    test_df\n",
    "    .merge(\n",
    "        products_df[['partnumber', 'global_success_rate']], \n",
    "        on='partnumber', \n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Ponderamos entre predicción y popularidad\n",
    "PREDICTION_WEIGHT = 0.8\n",
    "GLOBAL_WEIGHT = 0.2\n",
    "\n",
    "enriched_test['composite_score'] = (\n",
    "    PREDICTION_WEIGHT * enriched_test['predicted_success_rate'] + \n",
    "    GLOBAL_WEIGHT * enriched_test['global_success_rate'].fillna(0)\n",
    ")\n",
    "\n",
    "# Generamos las recomendaciones iniciales\n",
    "recommendations = (\n",
    "    enriched_test\n",
    "    .sort_values(['composite_score'], ascending=[False])\n",
    "    .groupby('session_id')\n",
    "    .agg({\n",
    "        'partnumber': lambda x: list(x)\n",
    "    }) \n",
    ")\n",
    "\n",
    "result_dict = {\n",
    "    \"target\": {\n",
    "        str(session_id)[:-1]: pad_recommendations(prods)  # Mantiene el flujo correcto\n",
    "        for session_id, prods in recommendations['partnumber'].items()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convertimos los valores a int justo antes de guardar el JSON\n",
    "for session_id in result_dict[\"target\"]:\n",
    "    result_dict[\"target\"][session_id] = [int(p) for p in result_dict[\"target\"][session_id]]\n",
    "\n",
    "# Guardar JSON\n",
    "output_path = Path('../../predictions/predictions_3.json')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(result_dict, f, indent=4)\n",
    "\n",
    "# Guardar JSON con los 100 primeros valores\n",
    "sample_output_path = Path('../../predictions/predictions_3_sample.json')\n",
    "sample_dict = {k: result_dict[k] for k in list(result_dict)[:100]}\n",
    "\n",
    "with open(sample_output_path, 'w') as f:\n",
    "    json.dump(sample_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de recomendación y resultados V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para obtener productos similares\n",
    "def get_similar_products(partnumber, k=3):\n",
    "    # return list(dict.fromkeys(product_similarities.get(partnumber, [])))[:k]\n",
    "    if partnumber not in product_similarities:\n",
    "        return []\n",
    "    \n",
    "    section = products_df[products_df['partnumber'] == partnumber]['cod_section'].values[0]\n",
    "    similar_prods = product_similarities[partnumber]\n",
    "    \n",
    "    # Filtrar productos de la misma sección\n",
    "    same_section_prods = [\n",
    "        prod for prod in similar_prods\n",
    "        if products_df[products_df['partnumber'] == prod]['cod_section'].values[0] == section\n",
    "    ]\n",
    "    \n",
    "    # Ordenar por global_success_rate descendente\n",
    "    same_section_prods_sorted = sorted(\n",
    "        same_section_prods,\n",
    "        key=lambda x: products_df[products_df['partnumber'] == x]['global_success_rate'].values[0],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    return same_section_prods_sorted[:k]\n",
    "\n",
    "\n",
    "# Función de padding para asegurar 5 recomendaciones únicas\n",
    "def pad_recommendations(prods, session_id):\n",
    "    prods = list(dict.fromkeys(prods))  # Unicidad\n",
    "\n",
    "    if len(prods) >= 5:\n",
    "        return prods[:5]\n",
    "\n",
    "    session_products = enriched_test[enriched_test['session_id'] == session_id]\n",
    "    prods_with_scores = [\n",
    "        (\n",
    "            p, \n",
    "            session_products.loc[session_products['partnumber'] == p, 'predicted_success_rate'].iloc[0],\n",
    "            session_products.loc[session_products['partnumber'] == p, 'composite_score'].iloc[0]\n",
    "        )\n",
    "        for p in prods if p in session_products['partnumber'].values\n",
    "    ]\n",
    "\n",
    "    # Ordenar productos por predicción y puntuación combinada\n",
    "    top_prods = [p[0] for p in sorted(prods_with_scores, key=lambda x: (x[1], x[2]), reverse=True)[:4]]\n",
    "\n",
    "    # Añadir solo 1 producto similar por cada top producto para diversificación\n",
    "    similar_prods = []\n",
    "    for p in top_prods:\n",
    "        similar_prods.extend(get_similar_products(p, k=5))\n",
    "\n",
    "    combined_prods = list(dict.fromkeys(prods + similar_prods))  # Unicidad\n",
    "\n",
    "    if len(combined_prods) >= 5:\n",
    "        return combined_prods[:5]\n",
    "\n",
    "    # Evitar productos populares si ya hay productos diversos\n",
    "    remaining = [p for p in popular_products if p not in combined_prods][:5 - len(combined_prods)]\n",
    "    final_recommendations = combined_prods + remaining\n",
    "\n",
    "    return final_recommendations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Enriquecemos test_df con global_success_rate\n",
    "enriched_test = test_df.merge(products_df[['partnumber', 'global_success_rate']], on='partnumber', how='left')\n",
    "\n",
    "# Ponderamos entre predicción y popularidad\n",
    "PREDICTION_WEIGHT = 1\n",
    "GLOBAL_WEIGHT = 0\n",
    "\n",
    "enriched_test['composite_score'] = PREDICTION_WEIGHT * enriched_test['predicted_success_rate'] + GLOBAL_WEIGHT * enriched_test['global_success_rate'].fillna(0)\n",
    "\n",
    "# Generamos las recomendaciones iniciales\n",
    "recommendations = enriched_test.sort_values(['composite_score'], ascending=[False]).groupby('session_id').agg({'partnumber': lambda x: list(x)})\n",
    "\n",
    "# Contadores para el análisis\n",
    "als_count = 0\n",
    "model_count = 0\n",
    "similar_count = 0\n",
    "popular_count = 0\n",
    "total_count = 0\n",
    "\n",
    "# Generamos las recomendaciones finales con padding\n",
    "result_dict = {\n",
    "    \"target\": {}\n",
    "}\n",
    "\n",
    "for session_id, prods in recommendations['partnumber'].items():\n",
    "    final_recommendations = pad_recommendations(prods, session_id)\n",
    "    result_dict[\"target\"][str(session_id)[:-1]] = final_recommendations\n",
    "\n",
    "    # Inicializar categorías de conteo\n",
    "    assigned_to_model = set(prods) & set(final_recommendations)\n",
    "    remaining_prods = set(final_recommendations) - assigned_to_model\n",
    "\n",
    "    assigned_to_similar = set()\n",
    "    for p in assigned_to_model:\n",
    "        assigned_to_similar.update(get_similar_products(p, k=5))\n",
    "\n",
    "    assigned_to_similar &= remaining_prods  # Solo contar productos que aún no están en modelo\n",
    "    remaining_prods -= assigned_to_similar  # Eliminar productos similares del conjunto restante\n",
    "\n",
    "    assigned_to_popular = remaining_prods  # Lo que queda se considera popular\n",
    "\n",
    "    # Actualizar contadores con las asignaciones únicas\n",
    "    model_count += len(assigned_to_model)\n",
    "    similar_count += len(assigned_to_similar)\n",
    "    popular_count += len(assigned_to_popular)\n",
    "    total_count += len(final_recommendations)\n",
    "\n",
    "# Verificar si el total coincide\n",
    "assert model_count + similar_count + popular_count == total_count, \"Error: los conteos no coinciden\"\n",
    "\n",
    "# Convertimos los valores a int justo antes de guardar el JSON\n",
    "for session_id in result_dict[\"target\"]:\n",
    "    result_dict[\"target\"][session_id] = [int(p) for p in result_dict[\"target\"][session_id]]\n",
    "\n",
    "# Guardar JSON\n",
    "output_path = Path('../../predictions/predictions_3.json')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(result_dict, f, indent=4)\n",
    "\n",
    "# Guardar JSON con los 100 primeros valores\n",
    "sample_output_path = Path('../../predictions/predictions_3_sample.json')\n",
    "sample_dict = {k: result_dict[k] for k in list(result_dict)[:100]}\n",
    "\n",
    "with open(sample_output_path, 'w') as f:\n",
    "    json.dump(sample_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Productos recomendados mediante ALS: 0\n",
      "Productos recomendados mediante el modelo de recomendación: 20312\n",
      "Productos recomendados mediante productos similares: 15626\n",
      "Productos recomendados mediante productos populares: 807\n",
      "Total de productos recomendados: 36745\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el análisis\n",
    "print(f\"Productos recomendados mediante ALS: {als_count}\")\n",
    "print(f\"Productos recomendados mediante el modelo de recomendación: {model_count}\")\n",
    "print(f\"Productos recomendados mediante productos similares: {similar_count}\")\n",
    "print(f\"Productos recomendados mediante productos populares: {popular_count}\")\n",
    "print(f\"Total de productos recomendados: {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos las recomendaciones basadas en productos similares\n",
    "def evaluate_similarity(test_df, k=5):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for session_id, purchased_product in zip(test_df['session_id'], test_df['partnumber']):\n",
    "        recommended_products = product_similarities.get(purchased_product, [])[:k]\n",
    "        \n",
    "        if purchased_product in recommended_products:\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        total_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    print(f\"Precisión de recomendaciones basadas en similitud: {accuracy:.4f}\")\n",
    "\n",
    "evaluate_similarity(test_df)  # Ver si aumentan las predicciones correctas\n",
    "\n",
    "# Comprobación manual de recomendaciones filtradas por sección\n",
    "sample_partnumber = products_df.sample(1)['partnumber'].iloc[0]\n",
    "print(f\"Producto original: {sample_partnumber}\")\n",
    "print(f\"Recomendaciones: {get_similar_products(sample_partnumber, k=5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir session_id de test_df a string y restar el último dígito\n",
    "test_session_ids = test_df['session_id'].astype(str).apply(lambda x: x[:-1]).unique()\n",
    "\n",
    "# Verificar que todos los session_id de test_df están en result_dict\n",
    "missing_session_ids = set(test_session_ids) - set(result_dict[\"target\"].keys())\n",
    "if missing_session_ids:\n",
    "    print(f\"Faltan los siguientes session_id en result_dict: {missing_session_ids}\")\n",
    "else:\n",
    "    print(\"Todos los session_id de test_df están presentes en result_dict.\")\n",
    "\n",
    "# Verificar que todas las recomendaciones tienen 5 productos diferentes\n",
    "incorrect_recommendations = []\n",
    "for session_id, recommendations in result_dict[\"target\"].items():\n",
    "    if len(recommendations) != 5:\n",
    "        incorrect_recommendations.append((session_id, recommendations))\n",
    "    elif len(recommendations) != len(set(recommendations)):\n",
    "        incorrect_recommendations.append((session_id, recommendations))\n",
    "\n",
    "if incorrect_recommendations:\n",
    "    print(f\"Las siguientes recomendaciones no tienen 5 productos únicos: {incorrect_recommendations}\")\n",
    "else:\n",
    "    print(\"Todas las recomendaciones tienen 5 productos únicos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo: Versión 3 \n",
    "### (fallida: ALS es buen modelo para sistemas de recomendacion pero no sirve para dar recomendaciones a usuarios/sesiones nuevas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def prepare_features(df, users_df, products_df):\n",
    "    # Convertir timestamps\n",
    "    df[\"hour\"] = pd.to_datetime(df[\"timestamp_local\"]).dt.hour.astype(\"int8\")\n",
    "\n",
    "    # Merge con users_df y products_df\n",
    "    df = df.merge(users_df[[\"user_id\", \"country\", \"R\", \"F\", \"M\", \"avg_success_rate_per_user\", \"num_unique_products_interacted\"]], \n",
    "                  on=[\"user_id\", \"country\"], how=\"left\")\n",
    "    df = df.merge(products_df[[\"partnumber\", \"discount\", \"family\", \"color_id\", \"cod_section\", \"total_adds_to_cart\", \"global_success_rate\"]], \n",
    "                  on=\"partnumber\", how=\"left\")\n",
    "\n",
    "    # Convertir variables categóricas a números\n",
    "    for col in [\"country\", \"device_type\", \"pagetype\", \"family\", \"color_id\", \"cod_section\"]:\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "    features = [\"session_id\", \"partnumber\", \"hour\", \"country\", \"device_type\", \"pagetype\", \n",
    "                \"family\", \"color_id\", \"cod_section\", \"total_adds_to_cart\", \"global_success_rate\", \n",
    "                \"avg_success_rate_per_user\", \"num_unique_products_interacted\", \"R\", \"F\", \"M\", \"discount\"]\n",
    "\n",
    "    return df[features]\n",
    "\n",
    "# Preparar los datos de entrenamiento y prueba\n",
    "X_train = prepare_features(train_df, users_df, products_df)\n",
    "y_train = train_df['success_rate']\n",
    "X_test = prepare_features(test_df, users_df, products_df)\n",
    "\n",
    "# Mapear user_id y partnumber a índices consecutivos\n",
    "session_mapping = {user: idx for idx, user in enumerate(X_train[\"session_id\"].unique())}\n",
    "product_mapping = {prod: idx for idx, prod in enumerate(X_train[\"partnumber\"].unique())}\n",
    "\n",
    "X_train[\"session_idx\"] = X_train[\"session_id\"].map(session_mapping)\n",
    "X_train[\"product_idx\"] = X_train[\"partnumber\"].map(product_mapping)\n",
    "\n",
    "# Crear matriz de usuario-producto basada en adiciones al carrito\n",
    "interaction_matrix = coo_matrix(\n",
    "    (y_train, (X_train[\"session_idx\"], X_train[\"product_idx\"])),\n",
    "    shape=(len(session_mapping), len(product_mapping))\n",
    ")\n",
    "\n",
    "# Convertir la matriz a formato CSR para indexación eficiente\n",
    "interaction_matrix = interaction_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo ALS\n",
    "als_model = AlternatingLeastSquares(factors=100, regularization=0.1, iterations=20)\n",
    "als_model.fit(interaction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejar valores nulos en embeddings\n",
    "products_df['embedding'] = products_df['embedding'].apply(lambda x: x if x is not None else np.zeros(1280))\n",
    "\n",
    "# Convertir a una matriz de NumPy\n",
    "product_embeddings = np.vstack(products_df[\"embedding\"].values)\n",
    "\n",
    "# Calcular similitud de productos\n",
    "product_similarity = cosine_similarity(product_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener productos similares\n",
    "def get_similar_products(product_id, top_n=3):\n",
    "    if product_id not in product_mapping:\n",
    "        return []\n",
    "    product_idx = product_mapping[product_id]\n",
    "    similar_idxs = np.argsort(product_similarity[product_idx])[::-1][1:top_n+1]\n",
    "    return [list(product_mapping.keys())[i] for i in similar_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar recomendaciones por session_id\n",
    "test_sessions = test_df[\"session_id\"].unique()\n",
    "recommendations = {}\n",
    "\n",
    "# Obtener productos más populares ordenados por global_success_rate\n",
    "popular_products = products_df.sort_values(by=\"global_success_rate\", ascending=False)[\"partnumber\"].tolist()\n",
    "\n",
    "for session in test_sessions:\n",
    "    if session not in session_mapping:\n",
    "        recommendations[session] = []  # Si no está en el mapeo, no podemos recomendar\n",
    "        continue\n",
    "\n",
    "    session_idx = session_mapping[session]\n",
    "\n",
    "    # Obtener recomendaciones del modelo ALS\n",
    "    recommended_product_idxs, _ = als_model.recommend(session_idx, interaction_matrix[session_idx], N=5)\n",
    "\n",
    "    # Convertir índices de productos a IDs reales\n",
    "    recommended_products = [list(product_mapping.keys())[idx] for idx in recommended_product_idxs]\n",
    "\n",
    "    # Mejorar recomendaciones con productos similares\n",
    "    final_recommendations = []\n",
    "    for product in recommended_products:\n",
    "        final_recommendations.append(product)  # Añadir el producto recomendado\n",
    "        final_recommendations.extend(get_similar_products(product, top_n=2))  # Añadir 2 productos similares\n",
    "    \n",
    "    # Eliminar duplicados y limitar a 5 productos\n",
    "    recommendations[str(session)] = list(dict.fromkeys(final_recommendations))[:5]\n",
    "\n",
    "    # Asignar recomendaciones a la sesión\n",
    "    # recommendations[str(session)] = recommended_products  # Usar str() sin modificar el ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir session_id a string y product IDs a int\n",
    "recommendations = {\n",
    "    str(session)[:-1]: [int(product) for product in products] \n",
    "    for session, products in recommendations.items()\n",
    "}\n",
    "\n",
    "# Guardar JSON\n",
    "output_path = Path('../../predictions/predictions_3.json')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump({\"target\": recommendations}, f, indent=4)\n",
    "\n",
    "print(\"Recomendaciones generadas en predictions_3.json\")\n",
    "\n",
    "# Guardar JSON con los 100 primeros valores\n",
    "sample_output_path = Path('../../predictions/predictions_3_sample.json')\n",
    "sample_recommendations = {k: recommendations[k] for k in list(recommendations)[:100]}\n",
    "\n",
    "with open(sample_output_path, \"w\") as f:\n",
    "    json.dump({\"target\": sample_recommendations}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que todos los session_id de test_df están en recommendations\n",
    "missing_sessions = set(map(lambda x: str(x)[:-1], test_df[\"session_id\"])) - set(recommendations.keys())\n",
    "if missing_sessions:\n",
    "    print(f\"⚠️ Faltan {len(missing_sessions)} session_id en las recomendaciones.\")\n",
    "\n",
    "# Verificar que cada session_id tiene exactamente 5 productos y no hay duplicados\n",
    "errors = []\n",
    "for session, products in recommendations.items():\n",
    "    if len(products) != 5:\n",
    "        errors.append(f\"❌ {session} tiene {len(products)} productos en lugar de 5.\")\n",
    "    if len(products) != len(set(products)):\n",
    "        errors.append(f\"❌ {session} tiene productos duplicados: {products}\")\n",
    "\n",
    "# Mostrar errores si los hay\n",
    "if errors:\n",
    "    print(\"\\n\".join(errors))\n",
    "else:\n",
    "    print(\"✅ Todas las recomendaciones tienen 5 productos únicos por sesión.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
