{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook para desarrollar la Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sistema de recomendación de 5 productos para cada sesión nueva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de los DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path    \n",
    "import pandas as pd\n",
    "\n",
    "RAW_DATA_PATH = Path('../../data/raw')\n",
    "\n",
    "train_df = pd.read_csv(         RAW_DATA_PATH / 'train.csv')\n",
    "test_df = pd.read_csv(          RAW_DATA_PATH / 'test.csv')\n",
    "products_df = pd.read_pickle(   RAW_DATA_PATH / 'products.pkl')\n",
    "users_df = pd.read_csv(         RAW_DATA_PATH / 'users_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Extraer datos del string (JSON) de la columna 'values'\n",
    "def extract_data_from_string(df, column_name):\n",
    "    \n",
    "    df[column_name] = df[column_name].apply(ast.literal_eval)\n",
    "    \n",
    "    df['country'] = df[column_name].apply(lambda x: x['country'])\n",
    "    df['R'] = df[column_name].apply(lambda x: x['R'])\n",
    "    df['F'] = df[column_name].apply(lambda x: x['F'])\n",
    "    df['M'] = df[column_name].apply(lambda x: x['M'])\n",
    "    \n",
    "    df = df.drop(columns=[column_name])\n",
    "    \n",
    "    df = df.explode(['country', 'R', 'F', 'M'])\n",
    "    \n",
    "    df['country'] = df['country'].astype(int)\n",
    "    df['R'] = df['R'].astype(int)\n",
    "    df['F'] = df['F'].astype(int)\n",
    "    df['M'] = df['M'].astype(float)\n",
    "    \n",
    "    return df\n",
    "\n",
    "users_df = extract_data_from_string(users_df, 'values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users DataFrame dtypes:\n",
      " user_id      int64\n",
      "country      int32\n",
      "R            int32\n",
      "F            int32\n",
      "M          float64\n",
      "dtype: object\n",
      "\n",
      "Train DataFrame dtypes:\n",
      " session_id                  int64\n",
      "date                       object\n",
      "timestamp_local    datetime64[ns]\n",
      "add_to_cart                 int64\n",
      "user_id                     int32\n",
      "country                     int64\n",
      "partnumber                  int64\n",
      "device_type                 int64\n",
      "pagetype                    int32\n",
      "dtype: object\n",
      "\n",
      "Test DataFrame dtypes:\n",
      " session_id                  int64\n",
      "date                       object\n",
      "timestamp_local    datetime64[ns]\n",
      "user_id                     int32\n",
      "country                     int64\n",
      "partnumber                  int64\n",
      "device_type                 int64\n",
      "pagetype                    int32\n",
      "dtype: object\n",
      "\n",
      "Products DataFrame dtypes:\n",
      " discount        int32\n",
      "embedding      object\n",
      "partnumber      int32\n",
      "color_id        int32\n",
      "cod_section     int32\n",
      "family          int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Products DataFrame Processing\n",
    "products_df['discount'] = products_df['discount'].astype(int)\n",
    "products_df['cod_section'] = products_df['cod_section'].fillna(products_df['cod_section'].median()).astype(int)\n",
    "\n",
    "# Train DataFrame Processing\n",
    "train_df['user_id'] = train_df['user_id'].fillna(0).astype(int)\n",
    "train_df['timestamp_local'] = pd.to_datetime(train_df['timestamp_local'])\n",
    "train_df['pagetype'] = train_df['pagetype'].fillna(train_df['pagetype'].mode()[0]).astype(int)\n",
    "\n",
    "test_df['user_id'] = test_df['user_id'].fillna(0).astype(int)\n",
    "test_df['timestamp_local'] = pd.to_datetime(test_df['timestamp_local'])\n",
    "test_df['pagetype'] = test_df['pagetype'].fillna(test_df['pagetype'].mode()[0]).astype(int)\n",
    "\n",
    "# Verify conversions\n",
    "print(\"Users DataFrame dtypes:\\n\", users_df.dtypes)\n",
    "print(\"\\nTrain DataFrame dtypes:\\n\", train_df.dtypes)\n",
    "print(\"\\nTest DataFrame dtypes:\\n\", test_df.dtypes)\n",
    "print(\"\\nProducts DataFrame dtypes:\\n\", products_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users DataFrame dtypes:\n",
      " user_id       int64\n",
      "country    category\n",
      "R             int32\n",
      "F             int32\n",
      "M           float64\n",
      "dtype: object\n",
      "\n",
      "Train DataFrame dtypes:\n",
      " session_id                  int64\n",
      "date                       object\n",
      "timestamp_local    datetime64[ns]\n",
      "add_to_cart                 int64\n",
      "user_id                     int32\n",
      "country                  category\n",
      "partnumber                  int64\n",
      "device_type              category\n",
      "pagetype                 category\n",
      "dtype: object\n",
      "\n",
      "Test DataFrame dtypes:\n",
      " session_id                  int64\n",
      "date                       object\n",
      "timestamp_local    datetime64[ns]\n",
      "user_id                     int32\n",
      "country                  category\n",
      "partnumber                  int64\n",
      "device_type              category\n",
      "pagetype                 category\n",
      "dtype: object\n",
      "\n",
      "Products DataFrame dtypes:\n",
      " discount          int32\n",
      "embedding        object\n",
      "partnumber        int32\n",
      "color_id       category\n",
      "cod_section    category\n",
      "family         category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convertimos las variables a categoricas, nos conviene para que el modelo las interprete como tal\n",
    "users_df['country'] = users_df['country'].astype('category')\n",
    "\n",
    "categorical_cols_train = ['device_type', 'pagetype', 'country']\n",
    "for col in categorical_cols_train:\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].astype('category')\n",
    "\n",
    "categorical_cols_products = ['color_id', 'cod_section', 'family']\n",
    "for col in categorical_cols_products:\n",
    "    products_df[col] = products_df[col].astype('category')\n",
    "\n",
    "print(\"Users DataFrame dtypes:\\n\", users_df.dtypes)\n",
    "print(\"\\nTrain DataFrame dtypes:\\n\", train_df.dtypes)\n",
    "print(\"\\nTest DataFrame dtypes:\\n\", test_df.dtypes)\n",
    "print(\"\\nProducts DataFrame dtypes:\\n\", products_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  user_id\n",
      "0         641      641\n",
      "1        1171     1171\n",
      "2        1171     1171\n",
      "3        5791     5791\n",
      "4       12201  4807290\n",
      "   session_id  user_id\n",
      "0        7461     7461\n",
      "1        7461     7461\n",
      "2        7461     7461\n",
      "3        7461     7461\n",
      "4        7461     7461\n",
      "0    1803480\n",
      "1    1754230\n",
      "1    1754230\n",
      "2    1803490\n",
      "3    1803500\n",
      "Name: user_id, dtype: int64\n",
      "\n",
      "Existen colisiones: False\n"
     ]
    }
   ],
   "source": [
    "# El user_id trae muchos valores nulos, por lo que se nos ocurre crear dos nuevos ids user_id, session_id sin colisiones entre ellos.\n",
    "# El objetivo es reeplazar los nulos en user_id por el session_id, de forma que si no tenemos el dato de usuario, al menos\n",
    "#     somos capaces de relacionar registros por sesión (supongamos, por ejemplo que son usuarios no registrados en la web)\n",
    "\n",
    "# Usamos columnas temporales para crear los nuevos\n",
    "users_df['temp_user_id'] = users_df['user_id'] * 10\n",
    "train_df['temp_session_id'] = train_df['session_id'] * 10 + 1\n",
    "train_df['temp_user_id'] = train_df['user_id'] * 10\n",
    "test_df['temp_session_id'] = test_df['session_id'] * 10 + 1\n",
    "test_df['temp_user_id'] = test_df['user_id'] * 10\n",
    "\n",
    "train_df.loc[train_df['temp_user_id'] == 0, 'temp_user_id'] = train_df.loc[train_df['temp_user_id'] == 0, 'temp_session_id'].astype('int32')\n",
    "test_df.loc[test_df['temp_user_id'] == 0, 'temp_user_id'] = test_df.loc[test_df['temp_user_id'] == 0, 'temp_session_id'].astype('int32')\n",
    "\n",
    "users_df['user_id'] = users_df['temp_user_id']\n",
    "train_df['session_id'] = train_df['temp_session_id']\n",
    "train_df['user_id'] = train_df['temp_user_id']\n",
    "test_df['session_id'] = test_df['temp_session_id']\n",
    "test_df['user_id'] = test_df['temp_user_id']\n",
    "\n",
    "users_df.drop('temp_user_id', axis=1, inplace=True)\n",
    "train_df.drop(['temp_session_id', 'temp_user_id'], axis=1, inplace=True)\n",
    "test_df.drop(['temp_session_id', 'temp_user_id'], axis=1, inplace=True)\n",
    "\n",
    "# Print, y verificar que no hay colisiones entre los ids.\n",
    "print(train_df[['session_id', 'user_id']].head())\n",
    "print(test_df[['session_id', 'user_id']].head())\n",
    "print(users_df['user_id'].head())\n",
    "\n",
    "print(\"\\nExisten colisiones:\", \n",
    "      bool(set(users_df['user_id']).intersection(set(train_df['session_id']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Añadimos variables de interés calculadas a nivel de user_id\"\"\"\n",
    "    \n",
    "    print(\"Transformamos timestamps...\")\n",
    "    df['total_seconds'] = df['timestamp_local'].astype(np.int64) // 1e9\n",
    "    df['hour'] = df['timestamp_local'].dt.hour\n",
    "    \n",
    "    print(\"Agrupamos por user_id...\")\n",
    "    grouped = df.groupby('user_id', observed=True).agg({\n",
    "        'partnumber': 'nunique',\n",
    "        'pagetype': 'nunique',\n",
    "        'total_seconds': ['min', 'max'],\n",
    "        'hour': 'first',\n",
    "        'user_id': 'size' # Interacciones\n",
    "    }).reset_index()\n",
    "    \n",
    "    grouped.columns = ['user_id', 'unique_products', 'unique_pagetypes', \n",
    "                      'min_time', 'max_time', 'first_interaction_hour',\n",
    "                      'total_interactions']\n",
    "    \n",
    "    # Calculamos el tiempo\n",
    "    grouped['total_user_time'] = (grouped['max_time'] - grouped['min_time']).round(2)\n",
    "    grouped = grouped.drop(['min_time', 'max_time'], axis=1)\n",
    "    \n",
    "    # Merge\n",
    "    print(\"Merging results...\")\n",
    "    result = df.merge(grouped, on='user_id')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformamos timestamps...\n",
      "Agrupamos por user_id...\n",
      "Merging results...\n",
      "Transformamos timestamps...\n",
      "Agrupamos por user_id...\n",
      "Merging results...\n"
     ]
    }
   ],
   "source": [
    "train_df = add_user_features(train_df)\n",
    "test_df = add_user_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadiremos tres columnas, el ratio de exito del usuario con el producto, el ratio de exito global del producto, y el numero de veces que se ha añadido al carrito globalmente cada producto.\n",
    "user_product_metrics = (\n",
    "    train_df\n",
    "    .groupby(['user_id', 'partnumber'])\n",
    "    .agg({\n",
    "        'add_to_cart': ['sum', 'count']\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "user_product_metrics.columns = ['user_id', 'partnumber', 'adds_to_cart', 'total_visits']\n",
    "user_product_metrics['success_rate'] = user_product_metrics['adds_to_cart'] / user_product_metrics['total_visits']\n",
    "\n",
    "train_df = train_df.merge(\n",
    "    user_product_metrics[['user_id', 'partnumber', 'success_rate']], \n",
    "    on=['user_id', 'partnumber'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "product_metrics = (\n",
    "    train_df\n",
    "    .groupby('partnumber')\n",
    "    .agg({\n",
    "        'add_to_cart': ['sum', 'count']\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "product_metrics.columns = ['partnumber', 'total_adds_to_cart', 'total_visits']\n",
    "product_metrics['global_success_rate'] = product_metrics['total_adds_to_cart'] / product_metrics['total_visits']\n",
    "\n",
    "products_df = products_df.merge(\n",
    "    product_metrics[['partnumber', 'total_adds_to_cart', 'global_success_rate']], \n",
    "    on='partnumber', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Cubrimos nulos\n",
    "train_df['success_rate'] = train_df['success_rate'].fillna(0)\n",
    "products_df['total_adds_to_cart'] = products_df['total_adds_to_cart'].fillna(0)\n",
    "products_df['global_success_rate'] = products_df['global_success_rate'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Train Analysis ====================\n",
      "\n",
      "Shape: (46551445, 17)\n",
      "Memory Usage: 4572.68 MB\n",
      "\n",
      "Columns and Types:\n",
      "session_id                         int64\n",
      "date                              object\n",
      "timestamp_local           datetime64[ns]\n",
      "add_to_cart                        int64\n",
      "user_id                            int32\n",
      "country                         category\n",
      "partnumber                         int64\n",
      "device_type                     category\n",
      "pagetype                        category\n",
      "total_seconds                    float64\n",
      "hour                               int32\n",
      "unique_products                    int64\n",
      "unique_pagetypes                   int64\n",
      "first_interaction_hour             int32\n",
      "total_interactions                 int64\n",
      "total_user_time                  float64\n",
      "success_rate                     float64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "session_id                0\n",
      "date                      0\n",
      "timestamp_local           0\n",
      "add_to_cart               0\n",
      "user_id                   0\n",
      "country                   0\n",
      "partnumber                0\n",
      "device_type               0\n",
      "pagetype                  0\n",
      "total_seconds             0\n",
      "hour                      0\n",
      "unique_products           0\n",
      "unique_pagetypes          0\n",
      "first_interaction_hour    0\n",
      "total_interactions        0\n",
      "total_user_time           0\n",
      "success_rate              0\n",
      "dtype: int64\n",
      "\n",
      "Numerical Columns Statistics:\n",
      "         session_id                timestamp_local   add_to_cart  \\\n",
      "count  4.655144e+07                       46551445  4.655144e+07   \n",
      "mean   2.582898e+07  2024-06-07 21:13:05.366069760  5.898384e-02   \n",
      "min    1.100000e+01     2024-06-01 02:00:00.051000  0.000000e+00   \n",
      "25%    1.289981e+07     2024-06-04 09:02:17.912000  0.000000e+00   \n",
      "50%    2.580033e+07     2024-06-07 18:59:48.132000  0.000000e+00   \n",
      "75%    3.875410e+07  2024-06-11 07:14:04.460999936  0.000000e+00   \n",
      "max    5.171848e+07     2024-06-16 22:11:17.725000  1.000000e+00   \n",
      "std    1.493335e+07                            NaN  2.355945e-01   \n",
      "\n",
      "            user_id    partnumber  total_seconds          hour  \\\n",
      "count  4.655144e+07  4.655144e+07   4.655144e+07  4.655144e+07   \n",
      "mean   2.244252e+07  2.171479e+04   1.717795e+09  1.272923e+01   \n",
      "min    1.000000e+01  1.000000e+00   1.717207e+09  0.000000e+00   \n",
      "25%    6.209971e+06  1.105200e+04   1.717492e+09  7.000000e+00   \n",
      "50%    2.135669e+07  2.164900e+04   1.717787e+09  1.400000e+01   \n",
      "75%    3.651441e+07  3.242300e+04   1.718090e+09  1.800000e+01   \n",
      "max    5.171848e+07  4.369200e+04   1.718576e+09  2.300000e+01   \n",
      "std    1.603881e+07  1.248680e+04   3.467587e+05  6.772460e+00   \n",
      "\n",
      "       unique_products  unique_pagetypes  first_interaction_hour  \\\n",
      "count     4.655144e+07      4.655144e+07            4.655144e+07   \n",
      "mean      5.697925e+01      1.079992e+00            1.273982e+01   \n",
      "min       1.000000e+00      1.000000e+00            0.000000e+00   \n",
      "25%       9.000000e+00      1.000000e+00            7.000000e+00   \n",
      "50%       2.200000e+01      1.000000e+00            1.400000e+01   \n",
      "75%       5.000000e+01      1.000000e+00            1.800000e+01   \n",
      "max       3.034000e+03      5.000000e+00            2.300000e+01   \n",
      "std       1.249830e+02      3.002696e-01            6.753687e+00   \n",
      "\n",
      "       total_interactions  total_user_time  success_rate  \n",
      "count        4.655144e+07     4.655144e+07  4.655144e+07  \n",
      "mean         7.544297e+01     3.107637e+04  5.898384e-02  \n",
      "min          1.000000e+00     0.000000e+00  0.000000e+00  \n",
      "25%          1.200000e+01     2.480000e+02  0.000000e+00  \n",
      "50%          2.900000e+01     7.310000e+02  0.000000e+00  \n",
      "75%          6.800000e+01     1.842000e+03  0.000000e+00  \n",
      "max          7.481000e+03     1.207829e+06  1.000000e+00  \n",
      "std          1.951096e+02     1.387644e+05  1.977661e-01  \n",
      "\n",
      "Sample Data:\n",
      "   session_id        date         timestamp_local  add_to_cart  user_id  \\\n",
      "0         641  2024-06-06 2024-06-06 16:43:17.389            0      641   \n",
      "1        1171  2024-06-08 2024-06-08 15:11:02.782            0     1171   \n",
      "2        1171  2024-06-08 2024-06-08 15:11:44.797            0     1171   \n",
      "3        5791  2024-06-05 2024-06-05 19:24:48.397            0     5791   \n",
      "4       12201  2024-06-04 2024-06-04 08:21:13.476            0  4807290   \n",
      "\n",
      "  country  partnumber device_type pagetype  total_seconds  hour  \\\n",
      "0      29       14327           1       24   1.717692e+09    16   \n",
      "1      57       38422           1       24   1.717859e+09    15   \n",
      "2      57       19763           1       24   1.717860e+09    15   \n",
      "3      29       30253           1       24   1.717615e+09    19   \n",
      "4      25        1592           1       24   1.717489e+09     8   \n",
      "\n",
      "   unique_products  unique_pagetypes  first_interaction_hour  \\\n",
      "0                1                 1                      16   \n",
      "1                2                 1                      15   \n",
      "2                2                 1                      15   \n",
      "3                1                 1                      19   \n",
      "4                2                 1                       8   \n",
      "\n",
      "   total_interactions  total_user_time  success_rate  \n",
      "0                   1              0.0           0.0  \n",
      "1                   2             42.0           0.0  \n",
      "2                   2             42.0           0.0  \n",
      "3                   1              0.0           0.0  \n",
      "4                   3              4.0           0.0  \n",
      "\n",
      "==================== Test Analysis ====================\n",
      "\n",
      "Shape: (29275, 15)\n",
      "Memory Usage: 2.43 MB\n",
      "\n",
      "Columns and Types:\n",
      "session_id                         int64\n",
      "date                              object\n",
      "timestamp_local           datetime64[ns]\n",
      "user_id                            int32\n",
      "country                         category\n",
      "partnumber                         int64\n",
      "device_type                     category\n",
      "pagetype                        category\n",
      "total_seconds                    float64\n",
      "hour                               int32\n",
      "unique_products                    int64\n",
      "unique_pagetypes                   int64\n",
      "first_interaction_hour             int32\n",
      "total_interactions                 int64\n",
      "total_user_time                  float64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "session_id                0\n",
      "date                      0\n",
      "timestamp_local           0\n",
      "user_id                   0\n",
      "country                   0\n",
      "partnumber                0\n",
      "device_type               0\n",
      "pagetype                  0\n",
      "total_seconds             0\n",
      "hour                      0\n",
      "unique_products           0\n",
      "unique_pagetypes          0\n",
      "first_interaction_hour    0\n",
      "total_interactions        0\n",
      "total_user_time           0\n",
      "dtype: int64\n",
      "\n",
      "Numerical Columns Statistics:\n",
      "         session_id                timestamp_local       user_id  \\\n",
      "count  2.927500e+04                          29275  2.927500e+04   \n",
      "mean   2.602346e+07  2024-06-16 01:07:58.761897472  2.137632e+07   \n",
      "min    7.461000e+03     2024-06-15 00:00:35.309000  2.060000e+03   \n",
      "25%    1.318920e+07  2024-06-15 13:45:43.255499776  4.814685e+06   \n",
      "50%    2.619119e+07  2024-06-16 00:16:00.219000064  1.949021e+07   \n",
      "75%    3.904801e+07     2024-06-16 14:08:17.384000  3.580889e+07   \n",
      "max    5.168933e+07     2024-06-16 23:55:50.519000  5.168933e+07   \n",
      "std    1.496322e+07                            NaN  1.630334e+07   \n",
      "\n",
      "         partnumber  total_seconds          hour  unique_products  \\\n",
      "count  29275.000000   2.927500e+04  29275.000000     29275.000000   \n",
      "mean   22085.171238   1.718500e+09     12.479453         5.910470   \n",
      "min        3.000000   1.718410e+09      0.000000         1.000000   \n",
      "25%    11544.500000   1.718459e+09      7.000000         5.000000   \n",
      "50%    21777.000000   1.718497e+09     13.000000         6.000000   \n",
      "75%    32864.000000   1.718547e+09     18.000000         8.000000   \n",
      "max    43679.000000   1.718582e+09     23.000000        19.000000   \n",
      "std    12502.857230   4.973854e+04      6.671686         2.536947   \n",
      "\n",
      "       unique_pagetypes  first_interaction_hour  total_interactions  \\\n",
      "count      29275.000000            29275.000000        29275.000000   \n",
      "mean           1.116687               12.459846            6.964065   \n",
      "min            1.000000                0.000000            1.000000   \n",
      "25%            1.000000                7.000000            6.000000   \n",
      "50%            1.000000               13.000000            8.000000   \n",
      "75%            1.000000               18.000000            9.000000   \n",
      "max            3.000000               23.000000           20.000000   \n",
      "std            0.337950                6.665255            2.851067   \n",
      "\n",
      "       total_user_time  \n",
      "count     29275.000000  \n",
      "mean        288.370692  \n",
      "min           0.000000  \n",
      "25%          54.000000  \n",
      "50%         125.000000  \n",
      "75%         240.000000  \n",
      "max       43509.000000  \n",
      "std        1655.978425  \n",
      "\n",
      "Sample Data:\n",
      "   session_id        date         timestamp_local  user_id country  \\\n",
      "0        7461  2024-06-15 2024-06-15 18:36:47.390     7461      57   \n",
      "1        7461  2024-06-15 2024-06-15 18:37:04.052     7461      57   \n",
      "2        7461  2024-06-15 2024-06-15 18:37:48.159     7461      57   \n",
      "3        7461  2024-06-15 2024-06-15 18:38:19.899     7461      57   \n",
      "4        7461  2024-06-15 2024-06-15 18:38:46.492     7461      57   \n",
      "\n",
      "   partnumber device_type pagetype  total_seconds  hour  unique_products  \\\n",
      "0        1254           1       24   1.718477e+09    18                7   \n",
      "1       32544           1       24   1.718477e+09    18                7   \n",
      "2       12639           1       24   1.718477e+09    18                7   \n",
      "3       18048           1       24   1.718477e+09    18                7   \n",
      "4       13295           1       24   1.718477e+09    18                7   \n",
      "\n",
      "   unique_pagetypes  first_interaction_hour  total_interactions  \\\n",
      "0                 1                      18                   7   \n",
      "1                 1                      18                   7   \n",
      "2                 1                      18                   7   \n",
      "3                 1                      18                   7   \n",
      "4                 1                      18                   7   \n",
      "\n",
      "   total_user_time  \n",
      "0            148.0  \n",
      "1            148.0  \n",
      "2            148.0  \n",
      "3            148.0  \n",
      "4            148.0  \n",
      "\n",
      "==================== Users Analysis ====================\n",
      "\n",
      "Shape: (577494, 5)\n",
      "Memory Usage: 18.18 MB\n",
      "\n",
      "Columns and Types:\n",
      "user_id       int64\n",
      "country    category\n",
      "R             int32\n",
      "F             int32\n",
      "M           float64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "user_id    0\n",
      "country    0\n",
      "R          0\n",
      "F          0\n",
      "M          0\n",
      "dtype: int64\n",
      "\n",
      "Numerical Columns Statistics:\n",
      "            user_id              R              F             M\n",
      "count  5.774940e+05  577494.000000  577494.000000  5.774940e+05\n",
      "mean   2.782990e+06      69.945702      36.632408  8.942316e+02\n",
      "std    1.607793e+06     142.504278      58.701908  6.821667e+04\n",
      "min    1.000000e+01       0.000000       0.000000  0.000000e+00\n",
      "25%    1.391002e+06       5.000000       8.000000  2.770545e+01\n",
      "50%    2.781605e+06      19.000000      22.000000  3.823774e+01\n",
      "75%    4.174768e+06      63.000000      47.000000  5.253000e+01\n",
      "max    5.570060e+06    1095.000000   10754.000000  2.898200e+07\n",
      "\n",
      "Sample Data:\n",
      "   user_id country    R   F          M\n",
      "0  1803480      25  171   5  87.088000\n",
      "1  1754230      25    4   6  58.760000\n",
      "1  1754230      46   55   2  74.375000\n",
      "2  1803490      25   13  22  31.946765\n",
      "3  1803500      25    6   3  17.948750\n",
      "\n",
      "==================== Products Analysis ====================\n",
      "\n",
      "Shape: (43692, 8)\n",
      "Memory Usage: 1.56 MB\n",
      "\n",
      "Columns and Types:\n",
      "discount                  int32\n",
      "embedding                object\n",
      "partnumber                int32\n",
      "color_id               category\n",
      "cod_section            category\n",
      "family                 category\n",
      "total_adds_to_cart      float64\n",
      "global_success_rate     float64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "discount                  0\n",
      "embedding              6170\n",
      "partnumber                0\n",
      "color_id                  0\n",
      "cod_section               0\n",
      "family                    0\n",
      "total_adds_to_cart        0\n",
      "global_success_rate       0\n",
      "dtype: int64\n",
      "\n",
      "Numerical Columns Statistics:\n",
      "           discount    partnumber  total_adds_to_cart  global_success_rate\n",
      "count  43692.000000  43692.000000        43692.000000         43692.000000\n",
      "mean       0.031539  21846.500000           62.844068             0.052117\n",
      "std        0.174771  12612.938317          156.303996             0.080079\n",
      "min        0.000000      1.000000            0.000000             0.000000\n",
      "25%        0.000000  10923.750000            0.000000             0.000000\n",
      "50%        0.000000  21846.500000            5.000000             0.038378\n",
      "75%        0.000000  32769.250000           51.000000             0.071429\n",
      "max        1.000000  43692.000000         2911.000000             1.000000\n",
      "\n",
      "Sample Data:\n",
      "   discount                                          embedding  partnumber  \\\n",
      "0         0  [-0.13401361, -0.1200429, -0.016117405, -0.167...       32776   \n",
      "1         0  [-0.0949274, -0.107294075, -0.16559914, -0.174...       41431   \n",
      "2         0  [-0.12904441, -0.07724628, -0.09799071, -0.164...       39419   \n",
      "3         1  [-0.12783332, -0.133868, -0.10101265, -0.18888...       36087   \n",
      "4         1  [-0.14092924, -0.1258284, -0.10809927, -0.1765...       34132   \n",
      "\n",
      "  color_id cod_section family  total_adds_to_cart  global_success_rate  \n",
      "0       85           4     73                 0.0             0.000000  \n",
      "1      135           4     73                 1.0             0.142857  \n",
      "2      339           4     73                 1.0             0.076923  \n",
      "3      135           4     73                 0.0             0.000000  \n",
      "4        3           4     73                 0.0             0.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_dataframes(dfs_dict):\n",
    "    for name, df in dfs_dict.items():\n",
    "        print(f\"\\n{'='*20} {name} Analysis {'='*20}\")\n",
    "        \n",
    "        print(f\"\\nShape: {df.shape}\")\n",
    "        print(f\"Memory Usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        print(\"\\nColumns and Types:\")\n",
    "        print(df.dtypes)\n",
    "        \n",
    "        print(\"\\nMissing Values:\")\n",
    "        print(df.isnull().sum())\n",
    "\n",
    "        print(\"\\nNumerical Columns Statistics:\")\n",
    "        print(df.describe())\n",
    "        \n",
    "        print(\"\\nSample Data:\")\n",
    "        print(df.head())\n",
    "\n",
    "dfs = {\n",
    "    'Train': train_df,\n",
    "    'Test': test_df,\n",
    "    'Users': users_df,\n",
    "    'Products': products_df\n",
    "}\n",
    "\n",
    "analyze_dataframes(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo: Versión 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.128495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1877\n",
      "[LightGBM] [Info] Number of data points in the train set: 46551445, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 0.058984\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def prepare_features(df, users_df, products_df):\n",
    "    \n",
    "    # Temporales\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local']).dt.hour.astype('int8')\n",
    "    df['day'] = pd.to_datetime(df['timestamp_local']).dt.day.astype('int8')\n",
    "    \n",
    "    # Merge de los datasets\n",
    "    df = df.merge(users_df[['user_id', 'country', 'R', 'F', 'M']], on=['user_id','country'], how='left')\n",
    "    df = df.merge(products_df[['partnumber', 'discount']], on='partnumber', how='left')\n",
    "    \n",
    "\n",
    "    features = ['hour', 'day', 'country', 'device_type', 'pagetype',\n",
    "                'R', 'F', 'M', 'discount', 'total_seconds',\n",
    "                'unique_products', 'unique_pagetypes', 'first_interaction_hour', 'total_interactions', 'total_user_time']\n",
    "\n",
    "    return df[features]\n",
    "\n",
    "# Prepare train and test data\n",
    "X_train = prepare_features(train_df, users_df, products_df)\n",
    "y_train = train_df['success_rate']\n",
    "X_test = prepare_features(test_df, users_df, products_df)\n",
    "\n",
    "print('Entrenando modelo...')\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediccion\n",
    "test_df['predicted_success_rate'] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Completaremos las recomendaciones con productos populares\n",
    "popular_products = (\n",
    "    products_df\n",
    "    .sort_values('global_success_rate', ascending=False)\n",
    "    ['partnumber']\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "enriched_test = (\n",
    "    test_df\n",
    "    .merge(\n",
    "        products_df[['partnumber', 'global_success_rate']], \n",
    "        on='partnumber', \n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Crearemos una composición para las recomendaciones, ponderando popularidad global y predicción\n",
    "PREDICTION_WEIGHT = 0.8\n",
    "GLOBAL_WEIGHT = 0.2\n",
    "\n",
    "enriched_test['composite_score'] = (\n",
    "    PREDICTION_WEIGHT * enriched_test['predicted_success_rate'] + \n",
    "    GLOBAL_WEIGHT * enriched_test['global_success_rate'].fillna(0)\n",
    ")\n",
    "\n",
    "recommendations = (\n",
    "    enriched_test\n",
    "    .sort_values(['composite_score'], ascending=[False])\n",
    "    .groupby('session_id')\n",
    "    .agg({\n",
    "        'partnumber': lambda x: list(x)\n",
    "    }) \n",
    ")\n",
    "\n",
    "def pad_recommendations(prods):\n",
    "    prods = list(dict.fromkeys(prods))  \n",
    "    \n",
    "    if len(prods) >= 5:\n",
    "        return prods[:5] \n",
    "    \n",
    "    remaining = [p for p in popular_products if p not in prods]\n",
    "    return prods + remaining[:5-len(prods)]\n",
    "\n",
    "result_dict = {\n",
    "    \"target\": {\n",
    "        str(session_id)[:-1]: pad_recommendations(prods)\n",
    "        for session_id, prods in recommendations['partnumber'].items()\n",
    "    }\n",
    "}\n",
    "\n",
    "for session_id, prods in result_dict['target'].items():\n",
    "    assert len(prods) == len(set(prods)) == 5, f\"Session {session_id} has duplicates or wrong length\"\n",
    "\n",
    "# Guardamos las predicciones en un archivo JSON\n",
    "output_path = Path('../../predictions/predictions_3.json')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(result_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Recomendaciones guardadas!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Entrenamiento de modelo rapido, no supervisado, de productos similares.\n",
    "product_features = products_df[['discount', 'total_adds_to_cart', 'global_success_rate','cod_section','family','color_id']].values\n",
    "nn_model = NearestNeighbors(n_neighbors=5, metric='cosine', algorithm='brute')\n",
    "nn_model.fit(product_features)\n",
    "\n",
    "distances, indices = nn_model.kneighbors(product_features)\n",
    "\n",
    "# Mapeamos cada valor de products_df con sus 5 K-vecinos del modelo anterior (lo usaremos en las recomendaciones)\n",
    "product_similarities = {\n",
    "    products_df.iloc[i]['partnumber']: products_df.iloc[indices[i]]['partnumber'].tolist()\n",
    "    for i in range(len(products_df))\n",
    "}\n",
    "\n",
    "def get_similar_products(partnumber, k=3):\n",
    "    return list(dict.fromkeys(product_similarities.get(partnumber, [])))[:k]\n",
    "\n",
    "def pad_recommendations(prods):\n",
    "    prods = list(dict.fromkeys(prods))  # Valores unicos\n",
    "    \n",
    "    if len(prods) >= 5:\n",
    "        return prods[:5]\n",
    "\n",
    "    # Si el modelo no es capaz de recomendar 5 productos, añadimos productos similares \n",
    "    similar_prods = []\n",
    "    for p in prods:\n",
    "        similar_prods.extend(get_similar_products(p, k=3))\n",
    "\n",
    "    combined_prods = list(dict.fromkeys(prods + similar_prods))  # Valores unicos\n",
    "\n",
    "    if len(combined_prods) >= 5:\n",
    "        return combined_prods[:5]\n",
    "\n",
    "    # Rellenamos con productos populares (puede suceder que no tengamos recomendaciones, por ejemplo un nuevo usuario, en ese caso recomendaremos productos que tienen un exito general)\n",
    "    remaining = [p for p in popular_products if p not in combined_prods]\n",
    "    final_recommendations = (combined_prods + remaining)[:5]\n",
    "\n",
    "    return final_recommendations\n",
    "\n",
    "result_dict = {\n",
    "    \"target\": {\n",
    "        str(session_id)[:-1]: pad_recommendations(prods)\n",
    "        for session_id, prods in recommendations['partnumber'].items()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Debugging\n",
    "for session_id, prods in result_dict['target'].items():\n",
    "    if len(prods) != len(set(prods)) or len(prods) != 5:\n",
    "        print(f\"⚠️ Debug: Session {session_id} - Recommendations: {prods}\")\n",
    "\n",
    "for session_id, prods in result_dict['target'].items():\n",
    "    assert len(prods) == len(set(prods)) == 5, f\"⚠️ AssertionError: Session {session_id} has duplicates or wrong length\"\n",
    "\n",
    "# Guardar JSON\n",
    "output_path = Path('../../predictions/predictions_3.json')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(result_dict, f, indent=4)\n",
    "\n",
    "print(\"✅ Recomendaciones guardadas!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo: Versión 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular historial del usuario en train_df\n",
    "user_history = (\n",
    "    train_df\n",
    "    .groupby('user_id')\n",
    "    .agg(\n",
    "        avg_success_rate_per_user=('success_rate', 'mean'),\n",
    "        num_unique_products_interacted=('partnumber', 'nunique')\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  avg_success_rate_per_user  num_unique_products_interacted\n",
      "0       10                       0.00                               5\n",
      "1       11                       0.00                               1\n",
      "2       20                       0.15                               7\n",
      "3       31                       0.00                               2\n",
      "4       40                       0.00                               4\n"
     ]
    }
   ],
   "source": [
    "print(user_history.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'user_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22960\\2469325793.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Preparar características\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musers_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproducts_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'success_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musers_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproducts_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\chans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10829\u001b[0m     ) -> DataFrame:\n\u001b[0;32m  10830\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10832\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\chans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         )\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\chans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\chans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1306\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m                         \u001b[1;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\chans\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'user_id'"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRanker\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_features(df, users_df, products_df):\n",
    "    \n",
    "    # Temporales\n",
    "    df['hour'] = pd.to_datetime(df['timestamp_local']).dt.hour.astype('int8')\n",
    "    df['day'] = pd.to_datetime(df['timestamp_local']).dt.day.astype('int8')\n",
    "    \n",
    "    # Merge de los datasets\n",
    "    df = df.merge(users_df[['user_id', 'country', 'R', 'F', 'M']], on=['user_id','country'], how='left')\n",
    "    df = df.merge(products_df[['partnumber', 'discount']], on='partnumber', how='left')\n",
    "\n",
    "    features = ['hour', 'day', 'country', 'device_type', 'pagetype',\n",
    "                'R', 'F', 'M', 'discount', 'total_seconds',\n",
    "                'unique_products', 'unique_pagetypes', 'first_interaction_hour', 'total_interactions', 'total_user_time']\n",
    "\n",
    "    return df[features]\n",
    "\n",
    "# Preparar características\n",
    "X_train = prepare_features(train_df, users_df, products_df)\n",
    "X_train = X_train.merge(user_history, on='user_id', how='left')\n",
    "y_train = train_df['success_rate']\n",
    "\n",
    "X_test = prepare_features(test_df, users_df, products_df)\n",
    "X_test = X_test.merge(user_history, on='user_id', how='left')\n",
    "\n",
    "# Definir grupos (número de interacciones por sesión)\n",
    "train_groups = train_df.groupby(\"user_id\").size().tolist()\n",
    "test_groups = test_df.groupby(\"user_id\").size().tolist()\n",
    "\n",
    "# Entrenar modelo de ranking\n",
    "model = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train, group=train_groups)\n",
    "\n",
    "# Predicción y ordenamiento\n",
    "test_df['predicted_success_rate'] = model.predict(X_test)\n",
    "test_df = test_df.sort_values(by=['user_id', 'predicted_success_rate'], ascending=[True, False])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
